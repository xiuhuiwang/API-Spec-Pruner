openapi: 3.0.0
info:
  title: OpenAI API
  description: The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference
    for more details.
  version: 2.3.0
  termsOfService: https://openai.com/policies/terms-of-use
  contact:
    name: OpenAI Support
    url: https://help.openai.com/
  license:
    name: MIT
    url: https://github.com/openai/openai-openapi/blob/master/LICENSE
servers:
- url: https://api.openai.com/v1
tags:
- name: Assistants
  description: Build Assistants that can call models and use tools.
- name: Audio
  description: Turn audio into text or text into audio.
- name: Chat
  description: Given a list of messages comprising a conversation, the model will
    return a response.
- name: Completions
  description: Given a prompt, the model will return one or more predicted completions,
    and can also return the probabilities of alternative tokens at each position.
- name: Embeddings
  description: Get a vector representation of a given input that can be easily consumed
    by machine learning models and algorithms.
- name: Fine-tuning
  description: Manage fine-tuning jobs to tailor a model to your specific training
    data.
- name: Batch
  description: Create large batches of API requests to run asynchronously.
- name: Files
  description: Files are used to upload documents that can be used with features like
    Assistants and Fine-tuning.
- name: Uploads
  description: Use Uploads to upload large files in multiple parts.
- name: Images
  description: Given a prompt and/or an input image, the model will generate a new
    image.
- name: Models
  description: List and describe the various models available in the API.
- name: Moderations
  description: Given text and/or image inputs, classifies if those inputs are potentially
    harmful.
- name: Audit Logs
  description: List user actions and configuration changes within this organization.
paths:
  /models:
    get:
      operationId: listModels
      tags:
      - Models
      summary: Lists the currently available models, and provides basic information
        about each one such as the owner and availability.
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListModelsResponse'
      x-oaiMeta:
        name: List models
        group: models
        returns: A list of [model](/docs/api-reference/models/object) objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/models \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              client.models.list()

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const list = await openai.models.list();\n\
              \n  for await (const model of list) {\n    console.log(model);\n  }\n\
              }\nmain();"
            csharp: "using System;\n\nusing OpenAI.Models;\n\nOpenAIModelClient client\
              \ = new(\n    apiKey: Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"\
              )\n);\n\nforeach (var model in client.GetModels().Value)\n{\n    Console.WriteLine(model.Id);\n\
              }\n"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"model-id-0\",\n      \"object\": \"model\",\n      \"created\": 1686935002,\n\
            \      \"owned_by\": \"organization-owner\"\n    },\n    {\n      \"id\"\
            : \"model-id-1\",\n      \"object\": \"model\",\n      \"created\": 1686935002,\n\
            \      \"owned_by\": \"organization-owner\",\n    },\n    {\n      \"\
            id\": \"model-id-2\",\n      \"object\": \"model\",\n      \"created\"\
            : 1686935002,\n      \"owned_by\": \"openai\"\n    },\n  ],\n  \"object\"\
            : \"list\"\n}\n"
  /chat/completions:
    post:
      operationId: createChatCompletion
      tags:
      - Chat
      summary: "**Starting a new project?** We recommend trying [Responses](/docs/api-reference/responses)\
        \ \nto take advantage of the latest OpenAI platform features. Compare\n[Chat\
        \ Completions with Responses](/docs/guides/responses-vs-chat-completions?api-mode=responses).\n\
        \n---\n\nCreates a model response for the given chat conversation. Learn more\
        \ in the\n[text generation](/docs/guides/text-generation), [vision](/docs/guides/vision),\n\
        and [audio](/docs/guides/audio) guides.\n\nParameter support can differ depending\
        \ on the model used to generate the\nresponse, particularly for newer reasoning\
        \ models. Parameters that are only\nsupported for reasoning models are noted\
        \ below. For the current state of \nunsupported parameters in reasoning models,\
        \ \n[refer to the reasoning guide](/docs/guides/reasoning).\n"
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateChatCompletionRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
            text/event-stream:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionStreamResponse'
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: 'Returns a [chat completion](/docs/api-reference/chat/object) object,
          or a streamed sequence of [chat completion chunk](/docs/api-reference/chat/streaming)
          objects if the request is streamed.

          '
        path: create
        examples:
        - title: Default
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_chat_model_id\",\n    \"messages\"\
              : [\n      {\n        \"role\": \"developer\",\n        \"content\"\
              : \"You are a helpful assistant.\"\n      },\n      {\n        \"role\"\
              : \"user\",\n        \"content\": \"Hello!\"\n      }\n    ]\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ncompletion =\
              \ client.chat.completions.create(\n  model=\"VAR_chat_model_id\",\n\
              \  messages=[\n    {\"role\": \"developer\", \"content\": \"You are\
              \ a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"\
              Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const completion = await openai.chat.completions.create({\n\
              \    messages: [{ role: \"developer\", content: \"You are a helpful\
              \ assistant.\" }],\n    model: \"VAR_chat_model_id\",\n    store: true,\n\
              \  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4o\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new SystemChatMessage(\"\
              You are a helpful assistant.\"),\n    new UserChatMessage(\"Hello!\"\
              )\n];\n\nChatCompletion completion = client.CompleteChat(messages);\n\
              \nConsole.WriteLine(completion.Content[0].Text);\n"
          response: "{\n  \"id\": \"chatcmpl-B9MBs8CjcvOU2jLn4n570S5qMJKcT\",\n  \"\
            object\": \"chat.completion\",\n  \"created\": 1741569952,\n  \"model\"\
            : \"gpt-4o-2024-08-06\",\n  \"choices\": [\n    {\n      \"index\": 0,\n\
            \      \"message\": {\n        \"role\": \"assistant\",\n        \"content\"\
            : \"Hello! How can I assist you today?\",\n        \"refusal\": null,\n\
            \        \"annotations\": []\n      },\n      \"logprobs\": null,\n  \
            \    \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"\
            prompt_tokens\": 19,\n    \"completion_tokens\": 10,\n    \"total_tokens\"\
            : 29,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n\
            \      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\"\
            : {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n    \
            \  \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\"\
            : 0\n    }\n  },\n  \"service_tier\": \"default\"\n}\n"
        - title: Image input
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n   \
              \   {\n        \"role\": \"user\",\n        \"content\": [\n       \
              \   {\n            \"type\": \"text\",\n            \"text\": \"What\
              \ is in this image?\"\n          },\n          {\n            \"type\"\
              : \"image_url\",\n            \"image_url\": {\n              \"url\"\
              : \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              \n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\"\
              : 300\n  }'\n"
            python: "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse =\
              \ client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n\
              \        {\n            \"role\": \"user\",\n            \"content\"\
              : [\n                {\"type\": \"text\", \"text\": \"What's in this\
              \ image?\"},\n                {\n                    \"type\": \"image_url\"\
              ,\n                    \"image_url\": {\n                        \"\
              url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              ,\n                    }\n                },\n            ],\n     \
              \   }\n    ],\n    max_tokens=300,\n)\n\nprint(response.choices[0])\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const response = await openai.chat.completions.create({\n\
              \    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"\
              user\",\n        content: [\n          { type: \"text\", text: \"What's\
              \ in this image?\" },\n          {\n            type: \"image_url\"\
              ,\n            image_url: {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              ,\n            },\n          }\n        ],\n      },\n    ],\n  });\n\
              \  console.log(response.choices[0]);\n}\nmain();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4o\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new UserChatMessage(\n\
              \    [\n        ChatMessageContentPart.CreateTextPart(\"What's in this\
              \ image?\"),\n        ChatMessageContentPart.CreateImagePart(new Uri(\"\
              https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              ))\n    ])\n];\n\nChatCompletion completion = client.CompleteChat(messages);\n\
              \nConsole.WriteLine(completion.Content[0].Text);\n"
          response: "{\n  \"id\": \"chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG\",\n  \"\
            object\": \"chat.completion\",\n  \"created\": 1741570283,\n  \"model\"\
            : \"gpt-4o-2024-08-06\",\n  \"choices\": [\n    {\n      \"index\": 0,\n\
            \      \"message\": {\n        \"role\": \"assistant\",\n        \"content\"\
            : \"The image shows a wooden boardwalk path running through a lush green\
            \ field or meadow. The sky is bright blue with some scattered clouds,\
            \ giving the scene a serene and peaceful atmosphere. Trees and shrubs\
            \ are visible in the background.\",\n        \"refusal\": null,\n    \
            \    \"annotations\": []\n      },\n      \"logprobs\": null,\n      \"\
            finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\"\
            : 1117,\n    \"completion_tokens\": 46,\n    \"total_tokens\": 1163,\n\
            \    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n     \
            \ \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n\
            \      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"\
            accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\"\
            : 0\n    }\n  },\n  \"service_tier\": \"default\"\n}\n"
        - title: Streaming
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_chat_model_id\",\n    \"messages\"\
              : [\n      {\n        \"role\": \"developer\",\n        \"content\"\
              : \"You are a helpful assistant.\"\n      },\n      {\n        \"role\"\
              : \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"\
              stream\": true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ncompletion =\
              \ client.chat.completions.create(\n  model=\"VAR_chat_model_id\",\n\
              \  messages=[\n    {\"role\": \"developer\", \"content\": \"You are\
              \ a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"\
              Hello!\"}\n  ],\n  stream=True\n)\n\nfor chunk in completion:\n  print(chunk.choices[0].delta)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const completion = await openai.chat.completions.create({\n\
              \    model: \"VAR_chat_model_id\",\n    messages: [\n      {\"role\"\
              : \"developer\", \"content\": \"You are a helpful assistant.\"},\n \
              \     {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    stream:\
              \ true,\n  });\n\n  for await (const chunk of completion) {\n    console.log(chunk.choices[0].delta.content);\n\
              \  }\n}\n\nmain();\n"
            csharp: "using System;\nusing System.ClientModel;\nusing System.Collections.Generic;\n\
              using System.Threading.Tasks;\n\nusing OpenAI.Chat;\n\nChatClient client\
              \ = new(\n    model: \"gpt-4o\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new SystemChatMessage(\"\
              You are a helpful assistant.\"),\n    new UserChatMessage(\"Hello!\"\
              )\n];\n\nAsyncCollectionResult<StreamingChatCompletionUpdate> completionUpdates\
              \ = client.CompleteChatStreamingAsync(messages);\n\nawait foreach (StreamingChatCompletionUpdate\
              \ completionUpdate in completionUpdates)\n{\n    if (completionUpdate.ContentUpdate.Count\
              \ > 0)\n    {\n        Console.Write(completionUpdate.ContentUpdate[0].Text);\n\
              \    }\n}\n"
          response: '{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


            {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


            ....


            {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}

            '
        - title: Functions
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n-H \"Content-Type:\
              \ application/json\" \\\n-H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n-d '{\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\n  \
              \    \"role\": \"user\",\n      \"content\": \"What is the weather like\
              \ in Boston today?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\"\
              : \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\"\
              ,\n        \"description\": \"Get the current weather in a given location\"\
              ,\n        \"parameters\": {\n          \"type\": \"object\",\n    \
              \      \"properties\": {\n            \"location\": {\n            \
              \  \"type\": \"string\",\n              \"description\": \"The city\
              \ and state, e.g. San Francisco, CA\"\n            },\n            \"\
              unit\": {\n              \"type\": \"string\",\n              \"enum\"\
              : [\"celsius\", \"fahrenheit\"]\n            }\n          },\n     \
              \     \"required\": [\"location\"]\n        }\n      }\n    }\n  ],\n\
              \  \"tool_choice\": \"auto\"\n}'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ntools = [\n \
              \ {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\"\
              : \"get_current_weather\",\n      \"description\": \"Get the current\
              \ weather in a given location\",\n      \"parameters\": {\n        \"\
              type\": \"object\",\n        \"properties\": {\n          \"location\"\
              : {\n            \"type\": \"string\",\n            \"description\"\
              : \"The city and state, e.g. San Francisco, CA\",\n          },\n  \
              \        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"\
              fahrenheit\"]},\n        },\n        \"required\": [\"location\"],\n\
              \      },\n    }\n  }\n]\nmessages = [{\"role\": \"user\", \"content\"\
              : \"What's the weather like in Boston today?\"}]\ncompletion = client.chat.completions.create(\n\
              \  model=\"VAR_chat_model_id\",\n  messages=messages,\n  tools=tools,\n\
              \  tool_choice=\"auto\"\n)\n\nprint(completion)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const messages = [{\"role\": \"user\",\
              \ \"content\": \"What's the weather like in Boston today?\"}];\n  const\
              \ tools = [\n      {\n        \"type\": \"function\",\n        \"function\"\
              : {\n          \"name\": \"get_current_weather\",\n          \"description\"\
              : \"Get the current weather in a given location\",\n          \"parameters\"\
              : {\n            \"type\": \"object\",\n            \"properties\":\
              \ {\n              \"location\": {\n                \"type\": \"string\"\
              ,\n                \"description\": \"The city and state, e.g. San Francisco,\
              \ CA\",\n              },\n              \"unit\": {\"type\": \"string\"\
              , \"enum\": [\"celsius\", \"fahrenheit\"]},\n            },\n      \
              \      \"required\": [\"location\"],\n          },\n        }\n    \
              \  }\n  ];\n\n  const response = await openai.chat.completions.create({\n\
              \    model: \"gpt-4o\",\n    messages: messages,\n    tools: tools,\n\
              \    tool_choice: \"auto\",\n  });\n\n  console.log(response);\n}\n\n\
              main();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4o\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nChatTool getCurrentWeatherTool = ChatTool.CreateFunctionTool(\n\
              \    functionName: \"get_current_weather\",\n    functionDescription:\
              \ \"Get the current weather in a given location\",\n    functionParameters:\
              \ BinaryData.FromString(\"\"\"\n        {\n            \"type\": \"\
              object\",\n            \"properties\": {\n                \"location\"\
              : {\n                    \"type\": \"string\",\n                   \
              \ \"description\": \"The city and state, e.g. San Francisco, CA\"\n\
              \                },\n                \"unit\": {\n                 \
              \   \"type\": \"string\",\n                    \"enum\": [ \"celsius\"\
              , \"fahrenheit\" ]\n                }\n            },\n            \"\
              required\": [ \"location\" ]\n        }\n    \"\"\")\n);\n\nList<ChatMessage>\
              \ messages =\n[\n    new UserChatMessage(\"What's the weather like in\
              \ Boston today?\"),\n];\n\nChatCompletionOptions options = new()\n{\n\
              \    Tools =\n    {\n        getCurrentWeatherTool\n    },\n    ToolChoice\
              \ = ChatToolChoice.CreateAutoChoice(),\n};\n\nChatCompletion completion\
              \ = client.CompleteChat(messages, options);\n"
          response: "{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\"\
            ,\n  \"created\": 1699896916,\n  \"model\": \"gpt-4o-mini\",\n  \"choices\"\
            : [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\"\
            : \"assistant\",\n        \"content\": null,\n        \"tool_calls\":\
            \ [\n          {\n            \"id\": \"call_abc123\",\n            \"\
            type\": \"function\",\n            \"function\": {\n              \"name\"\
            : \"get_current_weather\",\n              \"arguments\": \"{\\n\\\"location\\\
            \": \\\"Boston, MA\\\"\\n}\"\n            }\n          }\n        ]\n\
            \      },\n      \"logprobs\": null,\n      \"finish_reason\": \"tool_calls\"\
            \n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 82,\n    \"completion_tokens\"\
            : 17,\n    \"total_tokens\": 99,\n    \"completion_tokens_details\": {\n\
            \      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\"\
            : 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  }\n}\n"
        - title: Logprobs
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_chat_model_id\",\n    \"messages\"\
              : [\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\
              \n      }\n    ],\n    \"logprobs\": true,\n    \"top_logprobs\": 2\n\
              \  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ncompletion =\
              \ client.chat.completions.create(\n  model=\"VAR_chat_model_id\",\n\
              \  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n \
              \ ],\n  logprobs=True,\n  top_logprobs=2\n)\n\nprint(completion.choices[0].message)\n\
              print(completion.choices[0].logprobs)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const completion = await openai.chat.completions.create({\n\
              \    messages: [{ role: \"user\", content: \"Hello!\" }],\n    model:\
              \ \"VAR_chat_model_id\",\n    logprobs: true,\n    top_logprobs: 2,\n\
              \  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4o\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new UserChatMessage(\"\
              Hello!\")\n];\n\nChatCompletionOptions options = new()\n{\n    IncludeLogProbabilities\
              \ = true,\n    TopLogProbabilityCount = 2\n};\n\nChatCompletion completion\
              \ = client.CompleteChat(messages, options);\n\nConsole.WriteLine(completion.Content[0].Text);\n"
          response: "{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\"\
            ,\n  \"created\": 1702685778,\n  \"model\": \"gpt-4o-mini\",\n  \"choices\"\
            : [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\"\
            : \"assistant\",\n        \"content\": \"Hello! How can I assist you today?\"\
            \n      },\n      \"logprobs\": {\n        \"content\": [\n          {\n\
            \            \"token\": \"Hello\",\n            \"logprob\": -0.31725305,\n\
            \            \"bytes\": [72, 101, 108, 108, 111],\n            \"top_logprobs\"\
            : [\n              {\n                \"token\": \"Hello\",\n        \
            \        \"logprob\": -0.31725305,\n                \"bytes\": [72, 101,\
            \ 108, 108, 111]\n              },\n              {\n                \"\
            token\": \"Hi\",\n                \"logprob\": -1.3190403,\n         \
            \       \"bytes\": [72, 105]\n              }\n            ]\n       \
            \   },\n          {\n            \"token\": \"!\",\n            \"logprob\"\
            : -0.02380986,\n            \"bytes\": [\n              33\n         \
            \   ],\n            \"top_logprobs\": [\n              {\n           \
            \     \"token\": \"!\",\n                \"logprob\": -0.02380986,\n \
            \               \"bytes\": [33]\n              },\n              {\n \
            \               \"token\": \" there\",\n                \"logprob\": -3.787621,\n\
            \                \"bytes\": [32, 116, 104, 101, 114, 101]\n          \
            \    }\n            ]\n          },\n          {\n            \"token\"\
            : \" How\",\n            \"logprob\": -0.000054669687,\n            \"\
            bytes\": [32, 72, 111, 119],\n            \"top_logprobs\": [\n      \
            \        {\n                \"token\": \" How\",\n                \"logprob\"\
            : -0.000054669687,\n                \"bytes\": [32, 72, 111, 119]\n  \
            \            },\n              {\n                \"token\": \"<|end|>\"\
            ,\n                \"logprob\": -10.953937,\n                \"bytes\"\
            : null\n              }\n            ]\n          },\n          {\n  \
            \          \"token\": \" can\",\n            \"logprob\": -0.015801601,\n\
            \            \"bytes\": [32, 99, 97, 110],\n            \"top_logprobs\"\
            : [\n              {\n                \"token\": \" can\",\n         \
            \       \"logprob\": -0.015801601,\n                \"bytes\": [32, 99,\
            \ 97, 110]\n              },\n              {\n                \"token\"\
            : \" may\",\n                \"logprob\": -4.161023,\n               \
            \ \"bytes\": [32, 109, 97, 121]\n              }\n            ]\n    \
            \      },\n          {\n            \"token\": \" I\",\n            \"\
            logprob\": -3.7697225e-6,\n            \"bytes\": [\n              32,\n\
            \              73\n            ],\n            \"top_logprobs\": [\n \
            \             {\n                \"token\": \" I\",\n                \"\
            logprob\": -3.7697225e-6,\n                \"bytes\": [32, 73]\n     \
            \         },\n              {\n                \"token\": \" assist\"\
            ,\n                \"logprob\": -13.596657,\n                \"bytes\"\
            : [32, 97, 115, 115, 105, 115, 116]\n              }\n            ]\n\
            \          },\n          {\n            \"token\": \" assist\",\n    \
            \        \"logprob\": -0.04571125,\n            \"bytes\": [32, 97, 115,\
            \ 115, 105, 115, 116],\n            \"top_logprobs\": [\n            \
            \  {\n                \"token\": \" assist\",\n                \"logprob\"\
            : -0.04571125,\n                \"bytes\": [32, 97, 115, 115, 105, 115,\
            \ 116]\n              },\n              {\n                \"token\":\
            \ \" help\",\n                \"logprob\": -3.1089056,\n             \
            \   \"bytes\": [32, 104, 101, 108, 112]\n              }\n           \
            \ ]\n          },\n          {\n            \"token\": \" you\",\n   \
            \         \"logprob\": -5.4385737e-6,\n            \"bytes\": [32, 121,\
            \ 111, 117],\n            \"top_logprobs\": [\n              {\n     \
            \           \"token\": \" you\",\n                \"logprob\": -5.4385737e-6,\n\
            \                \"bytes\": [32, 121, 111, 117]\n              },\n  \
            \            {\n                \"token\": \" today\",\n             \
            \   \"logprob\": -12.807695,\n                \"bytes\": [32, 116, 111,\
            \ 100, 97, 121]\n              }\n            ]\n          },\n      \
            \    {\n            \"token\": \" today\",\n            \"logprob\": -0.0040071653,\n\
            \            \"bytes\": [32, 116, 111, 100, 97, 121],\n            \"\
            top_logprobs\": [\n              {\n                \"token\": \" today\"\
            ,\n                \"logprob\": -0.0040071653,\n                \"bytes\"\
            : [32, 116, 111, 100, 97, 121]\n              },\n              {\n  \
            \              \"token\": \"?\",\n                \"logprob\": -5.5247097,\n\
            \                \"bytes\": [63]\n              }\n            ]\n   \
            \       },\n          {\n            \"token\": \"?\",\n            \"\
            logprob\": -0.0008108172,\n            \"bytes\": [63],\n            \"\
            top_logprobs\": [\n              {\n                \"token\": \"?\",\n\
            \                \"logprob\": -0.0008108172,\n                \"bytes\"\
            : [63]\n              },\n              {\n                \"token\":\
            \ \"?\\n\",\n                \"logprob\": -7.184561,\n               \
            \ \"bytes\": [63, 10]\n              }\n            ]\n          }\n \
            \       ]\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n\
            \  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\"\
            : 9,\n    \"total_tokens\": 18,\n    \"completion_tokens_details\": {\n\
            \      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\"\
            : 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"system_fingerprint\"\
            : null\n}\n"
  /embeddings:
    post:
      operationId: createEmbedding
      tags:
      - Embeddings
      summary: Creates an embedding vector representing the input text.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateEmbeddingRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateEmbeddingResponse'
      x-oaiMeta:
        name: Create embeddings
        group: embeddings
        returns: A list of [embedding](/docs/api-reference/embeddings/object) objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/embeddings \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
              \ \\\n  -d '{\n    \"input\": \"The food was delicious and the waiter...\"\
              ,\n    \"model\": \"text-embedding-ada-002\",\n    \"encoding_format\"\
              : \"float\"\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nclient.embeddings.create(\n\
              \  model=\"text-embedding-ada-002\",\n  input=\"The food was delicious\
              \ and the waiter...\",\n  encoding_format=\"float\"\n)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const embedding = await openai.embeddings.create({\n\
              \    model: \"text-embedding-ada-002\",\n    input: \"The quick brown\
              \ fox jumped over the lazy dog\",\n    encoding_format: \"float\",\n\
              \  });\n\n  console.log(embedding);\n}\n\nmain();\n"
            csharp: "using System;\n\nusing OpenAI.Embeddings;\n\nEmbeddingClient\
              \ client = new(\n    model: \"text-embedding-3-small\",\n    apiKey:\
              \ Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\")\n);\n\nOpenAIEmbedding\
              \ embedding = client.GenerateEmbedding(input: \"The quick brown fox\
              \ jumped over the lazy dog\");\nReadOnlyMemory<float> vector = embedding.ToFloats();\n\
              \nfor (int i = 0; i < vector.Length; i++)\n{\n    Console.WriteLine($\"\
              \  [{i,4}] = {vector.Span[i]}\");\n}\n"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\"\
            : \"embedding\",\n      \"embedding\": [\n        0.0023064255,\n    \
            \    -0.009327292,\n        .... (1536 floats total for ada-002)\n   \
            \     -0.0028842222,\n      ],\n      \"index\": 0\n    }\n  ],\n  \"\
            model\": \"text-embedding-ada-002\",\n  \"usage\": {\n    \"prompt_tokens\"\
            : 8,\n    \"total_tokens\": 8\n  }\n}\n"
  /files:
    get:
      operationId: listFiles
      tags:
      - Files
      summary: Returns a list of files.
      parameters:
      - in: query
        name: purpose
        required: false
        schema:
          type: string
        description: Only return files with the given purpose.
      - name: limit
        in: query
        description: 'A limit on the number of objects to be returned. Limit can range
          between 1 and 10,000, and the default is 10,000.

          '
        required: false
        schema:
          type: integer
          default: 10000
      - name: order
        in: query
        description: 'Sort order by the `created_at` timestamp of the objects. `asc`
          for ascending order and `desc` for descending order.

          '
        schema:
          type: string
          default: desc
          enum:
          - asc
          - desc
      - name: after
        in: query
        description: 'A cursor for use in pagination. `after` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, ending with obj_foo, your subsequent call can include
          after=obj_foo in order to fetch the next page of the list.

          '
        schema:
          type: string
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListFilesResponse'
      x-oaiMeta:
        name: List files
        group: files
        returns: A list of [File](/docs/api-reference/files/object) objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/files \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              client.files.list()

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const list = await openai.files.list();\n\
              \n  for await (const file of list) {\n    console.log(file);\n  }\n\
              }\n\nmain();"
          response: "{\n  \"data\": [\n    {\n      \"id\": \"file-abc123\",\n   \
            \   \"object\": \"file\",\n      \"bytes\": 175,\n      \"created_at\"\
            : 1613677385,\n      \"filename\": \"salesOverview.pdf\",\n      \"purpose\"\
            : \"assistants\",\n    },\n    {\n      \"id\": \"file-abc123\",\n   \
            \   \"object\": \"file\",\n      \"bytes\": 140,\n      \"created_at\"\
            : 1613779121,\n      \"filename\": \"puppy.jsonl\",\n      \"purpose\"\
            : \"fine-tune\",\n    }\n  ],\n  \"object\": \"list\"\n}\n"
    post:
      operationId: createFile
      tags:
      - Files
      summary: 'Upload a file that can be used across various endpoints. Individual
        files can be up to 512 MB, and the size of all files uploaded by one organization
        can be up to 100 GB.


        The Assistants API supports files up to 2 million tokens and of specific file
        types. See the [Assistants Tools guide](/docs/assistants/tools) for details.


        The Fine-tuning API only supports `.jsonl` files. The input also has certain
        required formats for fine-tuning [chat](/docs/api-reference/fine-tuning/chat-input)
        or [completions](/docs/api-reference/fine-tuning/completions-input) models.


        The Batch API only supports `.jsonl` files up to 200 MB in size. The input
        also has a specific required [format](/docs/api-reference/batch/request-input).


        Please [contact us](https://help.openai.com/) if you need to increase these
        storage limits.

        '
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/CreateFileRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/OpenAIFile'
      x-oaiMeta:
        name: Upload file
        group: files
        returns: The uploaded [File](/docs/api-reference/files/object) object.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/files \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -F purpose=\"fine-tune\" \\\n  -F file=\"\
              @mydata.jsonl\"\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nclient.files.create(\n\
              \  file=open(\"mydata.jsonl\", \"rb\"),\n  purpose=\"fine-tune\"\n)\n"
            node.js: "import fs from \"fs\";\nimport OpenAI from \"openai\";\n\nconst\
              \ openai = new OpenAI();\n\nasync function main() {\n  const file =\
              \ await openai.files.create({\n    file: fs.createReadStream(\"mydata.jsonl\"\
              ),\n    purpose: \"fine-tune\",\n  });\n\n  console.log(file);\n}\n\n\
              main();"
          response: "{\n  \"id\": \"file-abc123\",\n  \"object\": \"file\",\n  \"\
            bytes\": 120000,\n  \"created_at\": 1677610602,\n  \"filename\": \"mydata.jsonl\"\
            ,\n  \"purpose\": \"fine-tune\",\n}\n"
  /files/{file_id}:
    delete:
      operationId: deleteFile
      tags:
      - Files
      summary: Delete a file.
      parameters:
      - in: path
        name: file_id
        required: true
        schema:
          type: string
        description: The ID of the file to use for this request.
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DeleteFileResponse'
      x-oaiMeta:
        name: Delete file
        group: files
        returns: Deletion status.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/files/file-abc123 \\\n  -X DELETE\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              client.files.delete("file-abc123")

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const file = await openai.files.del(\"\
              file-abc123\");\n\n  console.log(file);\n}\n\nmain();"
          response: "{\n  \"id\": \"file-abc123\",\n  \"object\": \"file\",\n  \"\
            deleted\": true\n}\n"
  /vector_stores:
    get:
      operationId: listVectorStores
      tags:
      - Vector stores
      summary: Returns a list of vector stores.
      parameters:
      - name: limit
        in: query
        description: 'A limit on the number of objects to be returned. Limit can range
          between 1 and 100, and the default is 20.

          '
        required: false
        schema:
          type: integer
          default: 20
      - name: order
        in: query
        description: 'Sort order by the `created_at` timestamp of the objects. `asc`
          for ascending order and `desc` for descending order.

          '
        schema:
          type: string
          default: desc
          enum:
          - asc
          - desc
      - name: after
        in: query
        description: 'A cursor for use in pagination. `after` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, ending with obj_foo, your subsequent call can include
          after=obj_foo in order to fetch the next page of the list.

          '
        schema:
          type: string
      - name: before
        in: query
        description: 'A cursor for use in pagination. `before` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, starting with obj_foo, your subsequent call can
          include before=obj_foo in order to fetch the previous page of the list.

          '
        schema:
          type: string
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListVectorStoresResponse'
      x-oaiMeta:
        name: List vector stores
        group: vector_stores
        returns: A list of [vector store](/docs/api-reference/vector-stores/object)
          objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/vector_stores \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
              \ \\\n  -H \"OpenAI-Beta: assistants=v2\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              vector_stores = client.vector_stores.list()

              print(vector_stores)

              '
            node.js: "import OpenAI from \"openai\";\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const vectorStores = await openai.vectorStores.list();\n\
              \  console.log(vectorStores);\n}\n\nmain();\n"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"vs_abc123\",\n      \"object\": \"vector_store\",\n      \"created_at\"\
            : 1699061776,\n      \"name\": \"Support FAQ\",\n      \"bytes\": 139920,\n\
            \      \"file_counts\": {\n        \"in_progress\": 0,\n        \"completed\"\
            : 3,\n        \"failed\": 0,\n        \"cancelled\": 0,\n        \"total\"\
            : 3\n      }\n    },\n    {\n      \"id\": \"vs_abc456\",\n      \"object\"\
            : \"vector_store\",\n      \"created_at\": 1699061776,\n      \"name\"\
            : \"Support FAQ v2\",\n      \"bytes\": 139920,\n      \"file_counts\"\
            : {\n        \"in_progress\": 0,\n        \"completed\": 3,\n        \"\
            failed\": 0,\n        \"cancelled\": 0,\n        \"total\": 3\n      }\n\
            \    }\n  ],\n  \"first_id\": \"vs_abc123\",\n  \"last_id\": \"vs_abc456\"\
            ,\n  \"has_more\": false\n}\n"
    post:
      operationId: createVectorStore
      tags:
      - Vector stores
      summary: Create a vector store.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateVectorStoreRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VectorStoreObject'
      x-oaiMeta:
        name: Create vector store
        group: vector_stores
        returns: A [vector store](/docs/api-reference/vector-stores/object) object.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/vector_stores \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
              \ \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"name\"\
              : \"Support FAQ\"\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nvector_store\
              \ = client.vector_stores.create(\n  name=\"Support FAQ\"\n)\nprint(vector_store)\n"
            node.js: "import OpenAI from \"openai\";\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const vectorStore = await openai.vectorStores.create({\n\
              \    name: \"Support FAQ\"\n  });\n  console.log(vectorStore);\n}\n\n\
              main();\n"
          response: "{\n  \"id\": \"vs_abc123\",\n  \"object\": \"vector_store\",\n\
            \  \"created_at\": 1699061776,\n  \"name\": \"Support FAQ\",\n  \"bytes\"\
            : 139920,\n  \"file_counts\": {\n    \"in_progress\": 0,\n    \"completed\"\
            : 3,\n    \"failed\": 0,\n    \"cancelled\": 0,\n    \"total\": 3\n  }\n\
            }\n"
  /vector_stores/{vector_store_id}/files:
    get:
      operationId: listVectorStoreFiles
      tags:
      - Vector stores
      summary: Returns a list of vector store files.
      parameters:
      - name: vector_store_id
        in: path
        description: The ID of the vector store that the files belong to.
        required: true
        schema:
          type: string
      - name: limit
        in: query
        description: 'A limit on the number of objects to be returned. Limit can range
          between 1 and 100, and the default is 20.

          '
        required: false
        schema:
          type: integer
          default: 20
      - name: order
        in: query
        description: 'Sort order by the `created_at` timestamp of the objects. `asc`
          for ascending order and `desc` for descending order.

          '
        schema:
          type: string
          default: desc
          enum:
          - asc
          - desc
      - name: after
        in: query
        description: 'A cursor for use in pagination. `after` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, ending with obj_foo, your subsequent call can include
          after=obj_foo in order to fetch the next page of the list.

          '
        schema:
          type: string
      - name: before
        in: query
        description: 'A cursor for use in pagination. `before` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, starting with obj_foo, your subsequent call can
          include before=obj_foo in order to fetch the previous page of the list.

          '
        schema:
          type: string
      - name: filter
        in: query
        description: Filter by file status. One of `in_progress`, `completed`, `failed`,
          `cancelled`.
        schema:
          type: string
          enum:
          - in_progress
          - completed
          - failed
          - cancelled
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListVectorStoreFilesResponse'
      x-oaiMeta:
        name: List vector store files
        group: vector_stores
        returns: A list of [vector store file](/docs/api-reference/vector-stores-files/file-object)
          objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/vector_stores/vs_abc123/files \\\
              \n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\"\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nvector_store_files\
              \ = client.vector_stores.files.list(\n  vector_store_id=\"vs_abc123\"\
              \n)\nprint(vector_store_files)\n"
            node.js: "import OpenAI from \"openai\";\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const vectorStoreFiles = await openai.vectorStores.files.list(\n\
              \    \"vs_abc123\"\n  );\n  console.log(vectorStoreFiles);\n}\n\nmain();\n"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"file-abc123\",\n      \"object\": \"vector_store.file\",\n      \"\
            created_at\": 1699061776,\n      \"vector_store_id\": \"vs_abc123\"\n\
            \    },\n    {\n      \"id\": \"file-abc456\",\n      \"object\": \"vector_store.file\"\
            ,\n      \"created_at\": 1699061776,\n      \"vector_store_id\": \"vs_abc123\"\
            \n    }\n  ],\n  \"first_id\": \"file-abc123\",\n  \"last_id\": \"file-abc456\"\
            ,\n  \"has_more\": false\n}\n"
    post:
      operationId: createVectorStoreFile
      tags:
      - Vector stores
      summary: Create a vector store file by attaching a [File](/docs/api-reference/files)
        to a [vector store](/docs/api-reference/vector-stores/object).
      parameters:
      - in: path
        name: vector_store_id
        required: true
        schema:
          type: string
          example: vs_abc123
        description: 'The ID of the vector store for which to create a File.

          '
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateVectorStoreFileRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VectorStoreFileObject'
      x-oaiMeta:
        name: Create vector store file
        group: vector_stores
        returns: A [vector store file](/docs/api-reference/vector-stores-files/file-object)
          object.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/vector_stores/vs_abc123/files \\\
              \n    -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n    -H \"Content-Type:\
              \ application/json\" \\\n    -H \"OpenAI-Beta: assistants=v2\" \\\n\
              \    -d '{\n      \"file_id\": \"file-abc123\"\n    }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nvector_store_file\
              \ = client.vector_stores.files.create(\n  vector_store_id=\"vs_abc123\"\
              ,\n  file_id=\"file-abc123\"\n)\nprint(vector_store_file)\n"
            node.js: "import OpenAI from \"openai\";\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const myVectorStoreFile = await openai.vectorStores.files.create(\n\
              \    \"vs_abc123\",\n    {\n      file_id: \"file-abc123\"\n    }\n\
              \  );\n  console.log(myVectorStoreFile);\n}\n\nmain();\n"
          response: "{\n  \"id\": \"file-abc123\",\n  \"object\": \"vector_store.file\"\
            ,\n  \"created_at\": 1699061776,\n  \"usage_bytes\": 1234,\n  \"vector_store_id\"\
            : \"vs_abcd\",\n  \"status\": \"completed\",\n  \"last_error\": null\n\
            }\n"
  /vector_stores/{vector_store_id}/files/{file_id}:
    delete:
      operationId: deleteVectorStoreFile
      tags:
      - Vector stores
      summary: Delete a vector store file. This will remove the file from the vector
        store but the file itself will not be deleted. To delete the file, use the
        [delete file](/docs/api-reference/files/delete) endpoint.
      parameters:
      - in: path
        name: vector_store_id
        required: true
        schema:
          type: string
        description: The ID of the vector store that the file belongs to.
      - in: path
        name: file_id
        required: true
        schema:
          type: string
        description: The ID of the file to delete.
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DeleteVectorStoreFileResponse'
      x-oaiMeta:
        name: Delete vector store file
        group: vector_stores
        returns: Deletion status
        examples:
          request:
            curl: "curl https://api.openai.com/v1/vector_stores/vs_abc123/files/file-abc123\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -X\
              \ DELETE\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ndeleted_vector_store_file\
              \ = client.vector_stores.files.delete(\n    vector_store_id=\"vs_abc123\"\
              ,\n    file_id=\"file-abc123\"\n)\nprint(deleted_vector_store_file)\n"
            node.js: "import OpenAI from \"openai\";\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const deletedVectorStoreFile = await openai.vectorStores.files.del(\n\
              \    \"vs_abc123\",\n    \"file-abc123\"\n  );\n  console.log(deletedVectorStoreFile);\n\
              }\n\nmain();\n"
          response: "{\n  id: \"file-abc123\",\n  object: \"vector_store.file.deleted\"\
            ,\n  deleted: true\n}\n"
  /assistants:
    get:
      operationId: listAssistants
      tags:
      - Assistants
      summary: Returns a list of assistants.
      parameters:
      - name: limit
        in: query
        description: 'A limit on the number of objects to be returned. Limit can range
          between 1 and 100, and the default is 20.

          '
        required: false
        schema:
          type: integer
          default: 20
      - name: order
        in: query
        description: 'Sort order by the `created_at` timestamp of the objects. `asc`
          for ascending order and `desc` for descending order.

          '
        schema:
          type: string
          default: desc
          enum:
          - asc
          - desc
      - name: after
        in: query
        description: 'A cursor for use in pagination. `after` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, ending with obj_foo, your subsequent call can include
          after=obj_foo in order to fetch the next page of the list.

          '
        schema:
          type: string
      - name: before
        in: query
        description: 'A cursor for use in pagination. `before` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, starting with obj_foo, your subsequent call can
          include before=obj_foo in order to fetch the previous page of the list.

          '
        schema:
          type: string
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListAssistantsResponse'
      x-oaiMeta:
        name: List assistants
        group: assistants
        beta: true
        returns: A list of [assistant](/docs/api-reference/assistants/object) objects.
        examples:
          request:
            curl: "curl \"https://api.openai.com/v1/assistants?order=desc&limit=20\"\
              \ \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v2\"\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nmy_assistants\
              \ = client.beta.assistants.list(\n    order=\"desc\",\n    limit=\"\
              20\",\n)\nprint(my_assistants.data)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const myAssistants = await openai.beta.assistants.list({\n\
              \    order: \"desc\",\n    limit: \"20\",\n  });\n\n  console.log(myAssistants.data);\n\
              }\n\nmain();"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"asst_abc123\",\n      \"object\": \"assistant\",\n      \"created_at\"\
            : 1698982736,\n      \"name\": \"Coding Tutor\",\n      \"description\"\
            : null,\n      \"model\": \"gpt-4o\",\n      \"instructions\": \"You are\
            \ a helpful assistant designed to make me better at coding!\",\n     \
            \ \"tools\": [],\n      \"tool_resources\": {},\n      \"metadata\": {},\n\
            \      \"top_p\": 1.0,\n      \"temperature\": 1.0,\n      \"response_format\"\
            : \"auto\"\n    },\n    {\n      \"id\": \"asst_abc456\",\n      \"object\"\
            : \"assistant\",\n      \"created_at\": 1698982718,\n      \"name\": \"\
            My Assistant\",\n      \"description\": null,\n      \"model\": \"gpt-4o\"\
            ,\n      \"instructions\": \"You are a helpful assistant designed to make\
            \ me better at coding!\",\n      \"tools\": [],\n      \"tool_resources\"\
            : {},\n      \"metadata\": {},\n      \"top_p\": 1.0,\n      \"temperature\"\
            : 1.0,\n      \"response_format\": \"auto\"\n    },\n    {\n      \"id\"\
            : \"asst_abc789\",\n      \"object\": \"assistant\",\n      \"created_at\"\
            : 1698982643,\n      \"name\": null,\n      \"description\": null,\n \
            \     \"model\": \"gpt-4o\",\n      \"instructions\": null,\n      \"\
            tools\": [],\n      \"tool_resources\": {},\n      \"metadata\": {},\n\
            \      \"top_p\": 1.0,\n      \"temperature\": 1.0,\n      \"response_format\"\
            : \"auto\"\n    }\n  ],\n  \"first_id\": \"asst_abc123\",\n  \"last_id\"\
            : \"asst_abc789\",\n  \"has_more\": false\n}\n"
  /threads:
    post:
      operationId: createThread
      tags:
      - Assistants
      summary: Create a thread.
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateThreadRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ThreadObject'
      x-oaiMeta:
        name: Create thread
        group: threads
        beta: true
        returns: A [thread](/docs/api-reference/threads) object.
        examples:
        - title: Empty
          request:
            curl: "curl https://api.openai.com/v1/threads \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d ''\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              empty_thread = client.beta.threads.create()

              print(empty_thread)

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const emptyThread = await openai.beta.threads.create();\n\
              \n  console.log(emptyThread);\n}\n\nmain();"
          response: "{\n  \"id\": \"thread_abc123\",\n  \"object\": \"thread\",\n\
            \  \"created_at\": 1699012949,\n  \"metadata\": {},\n  \"tool_resources\"\
            : {}\n}\n"
        - title: Messages
          request:
            curl: "curl https://api.openai.com/v1/threads \\\n-H \"Content-Type: application/json\"\
              \ \\\n-H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n-H \"OpenAI-Beta:\
              \ assistants=v2\" \\\n-d '{\n    \"messages\": [{\n      \"role\": \"\
              user\",\n      \"content\": \"Hello, what is AI?\"\n    }, {\n     \
              \ \"role\": \"user\",\n      \"content\": \"How does AI work? Explain\
              \ it in simple terms.\"\n    }]\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nmessage_thread\
              \ = client.beta.threads.create(\n  messages=[\n    {\n      \"role\"\
              : \"user\",\n      \"content\": \"Hello, what is AI?\"\n    },\n   \
              \ {\n      \"role\": \"user\",\n      \"content\": \"How does AI work?\
              \ Explain it in simple terms.\"\n    },\n  ]\n)\n\nprint(message_thread)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const messageThread = await openai.beta.threads.create({\n\
              \    messages: [\n      {\n        role: \"user\",\n        content:\
              \ \"Hello, what is AI?\"\n      },\n      {\n        role: \"user\"\
              ,\n        content: \"How does AI work? Explain it in simple terms.\"\
              ,\n      },\n    ],\n  });\n\n  console.log(messageThread);\n}\n\nmain();"
          response: "{\n  \"id\": \"thread_abc123\",\n  \"object\": \"thread\",\n\
            \  \"created_at\": 1699014083,\n  \"metadata\": {},\n  \"tool_resources\"\
            : {}\n}\n"
  /threads/{thread_id}/messages:
    get:
      operationId: listMessages
      tags:
      - Assistants
      summary: Returns a list of messages for a given thread.
      parameters:
      - in: path
        name: thread_id
        required: true
        schema:
          type: string
        description: The ID of the [thread](/docs/api-reference/threads) the messages
          belong to.
      - name: limit
        in: query
        description: 'A limit on the number of objects to be returned. Limit can range
          between 1 and 100, and the default is 20.

          '
        required: false
        schema:
          type: integer
          default: 20
      - name: order
        in: query
        description: 'Sort order by the `created_at` timestamp of the objects. `asc`
          for ascending order and `desc` for descending order.

          '
        schema:
          type: string
          default: desc
          enum:
          - asc
          - desc
      - name: after
        in: query
        description: 'A cursor for use in pagination. `after` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, ending with obj_foo, your subsequent call can include
          after=obj_foo in order to fetch the next page of the list.

          '
        schema:
          type: string
      - name: before
        in: query
        description: 'A cursor for use in pagination. `before` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, starting with obj_foo, your subsequent call can
          include before=obj_foo in order to fetch the previous page of the list.

          '
        schema:
          type: string
      - name: run_id
        in: query
        description: 'Filter messages by the run ID that generated them.

          '
        schema:
          type: string
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListMessagesResponse'
      x-oaiMeta:
        name: List messages
        group: threads
        beta: true
        returns: A list of [message](/docs/api-reference/messages) objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_abc123/messages \\\
              \n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v2\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              thread_messages = client.beta.threads.messages.list("thread_abc123")

              print(thread_messages.data)

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const threadMessages = await openai.beta.threads.messages.list(\n\
              \    \"thread_abc123\"\n  );\n\n  console.log(threadMessages.data);\n\
              }\n\nmain();"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"msg_abc123\",\n      \"object\": \"thread.message\",\n      \"created_at\"\
            : 1699016383,\n      \"assistant_id\": null,\n      \"thread_id\": \"\
            thread_abc123\",\n      \"run_id\": null,\n      \"role\": \"user\",\n\
            \      \"content\": [\n        {\n          \"type\": \"text\",\n    \
            \      \"text\": {\n            \"value\": \"How does AI work? Explain\
            \ it in simple terms.\",\n            \"annotations\": []\n          }\n\
            \        }\n      ],\n      \"attachments\": [],\n      \"metadata\":\
            \ {}\n    },\n    {\n      \"id\": \"msg_abc456\",\n      \"object\":\
            \ \"thread.message\",\n      \"created_at\": 1699016383,\n      \"assistant_id\"\
            : null,\n      \"thread_id\": \"thread_abc123\",\n      \"run_id\": null,\n\
            \      \"role\": \"user\",\n      \"content\": [\n        {\n        \
            \  \"type\": \"text\",\n          \"text\": {\n            \"value\":\
            \ \"Hello, what is AI?\",\n            \"annotations\": []\n         \
            \ }\n        }\n      ],\n      \"attachments\": [],\n      \"metadata\"\
            : {}\n    }\n  ],\n  \"first_id\": \"msg_abc123\",\n  \"last_id\": \"\
            msg_abc456\",\n  \"has_more\": false\n}\n"
    post:
      operationId: createMessage
      tags:
      - Assistants
      summary: Create a message.
      parameters:
      - in: path
        name: thread_id
        required: true
        schema:
          type: string
        description: The ID of the [thread](/docs/api-reference/threads) to create
          a message for.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateMessageRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/MessageObject'
      x-oaiMeta:
        name: Create message
        group: threads
        beta: true
        returns: A [message](/docs/api-reference/messages/object) object.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_abc123/messages \\\
              \n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\
              \n  -d '{\n      \"role\": \"user\",\n      \"content\": \"How does\
              \ AI work? Explain it in simple terms.\"\n    }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nthread_message\
              \ = client.beta.threads.messages.create(\n  \"thread_abc123\",\n  role=\"\
              user\",\n  content=\"How does AI work? Explain it in simple terms.\"\
              ,\n)\nprint(thread_message)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const threadMessages = await openai.beta.threads.messages.create(\n\
              \    \"thread_abc123\",\n    { role: \"user\", content: \"How does AI\
              \ work? Explain it in simple terms.\" }\n  );\n\n  console.log(threadMessages);\n\
              }\n\nmain();"
          response: "{\n  \"id\": \"msg_abc123\",\n  \"object\": \"thread.message\"\
            ,\n  \"created_at\": 1713226573,\n  \"assistant_id\": null,\n  \"thread_id\"\
            : \"thread_abc123\",\n  \"run_id\": null,\n  \"role\": \"user\",\n  \"\
            content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n  \
            \      \"value\": \"How does AI work? Explain it in simple terms.\",\n\
            \        \"annotations\": []\n      }\n    }\n  ],\n  \"attachments\"\
            : [],\n  \"metadata\": {}\n}\n"
  /threads/runs:
    post:
      operationId: createThreadAndRun
      tags:
      - Assistants
      summary: Create a thread and run it in one request.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateThreadAndRunRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunObject'
      x-oaiMeta:
        name: Create thread and run
        group: threads
        beta: true
        returns: A [run](/docs/api-reference/runs/object) object.
        examples:
        - title: Default
          request:
            curl: "curl https://api.openai.com/v1/threads/runs \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
              \ \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n      \"assistant_id\"\
              : \"asst_abc123\",\n      \"thread\": {\n        \"messages\": [\n \
              \         {\"role\": \"user\", \"content\": \"Explain deep learning\
              \ to a 5 year old.\"}\n        ]\n      }\n    }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nrun = client.beta.threads.create_and_run(\n\
              \  assistant_id=\"asst_abc123\",\n  thread={\n    \"messages\": [\n\
              \      {\"role\": \"user\", \"content\": \"Explain deep learning to\
              \ a 5 year old.\"}\n    ]\n  }\n)\n\nprint(run)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const run = await openai.beta.threads.createAndRun({\n\
              \    assistant_id: \"asst_abc123\",\n    thread: {\n      messages:\
              \ [\n        { role: \"user\", content: \"Explain deep learning to a\
              \ 5 year old.\" },\n      ],\n    },\n  });\n\n  console.log(run);\n\
              }\n\nmain();\n"
          response: "{\n  \"id\": \"run_abc123\",\n  \"object\": \"thread.run\",\n\
            \  \"created_at\": 1699076792,\n  \"assistant_id\": \"asst_abc123\",\n\
            \  \"thread_id\": \"thread_abc123\",\n  \"status\": \"queued\",\n  \"\
            started_at\": null,\n  \"expires_at\": 1699077392,\n  \"cancelled_at\"\
            : null,\n  \"failed_at\": null,\n  \"completed_at\": null,\n  \"required_action\"\
            : null,\n  \"last_error\": null,\n  \"model\": \"gpt-4o\",\n  \"instructions\"\
            : \"You are a helpful assistant.\",\n  \"tools\": [],\n  \"tool_resources\"\
            : {},\n  \"metadata\": {},\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n\
            \  \"max_completion_tokens\": null,\n  \"max_prompt_tokens\": null,\n\
            \  \"truncation_strategy\": {\n    \"type\": \"auto\",\n    \"last_messages\"\
            : null\n  },\n  \"incomplete_details\": null,\n  \"usage\": null,\n  \"\
            response_format\": \"auto\",\n  \"tool_choice\": \"auto\",\n  \"parallel_tool_calls\"\
            : true\n}\n"
        - title: Streaming
          request:
            curl: "curl https://api.openai.com/v1/threads/runs \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
              \ \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"assistant_id\"\
              : \"asst_123\",\n    \"thread\": {\n      \"messages\": [\n        {\"\
              role\": \"user\", \"content\": \"Hello\"}\n      ]\n    },\n    \"stream\"\
              : true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nstream = client.beta.threads.create_and_run(\n\
              \  assistant_id=\"asst_123\",\n  thread={\n    \"messages\": [\n   \
              \   {\"role\": \"user\", \"content\": \"Hello\"}\n    ]\n  },\n  stream=True\n\
              )\n\nfor event in stream:\n  print(event)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const stream = await openai.beta.threads.createAndRun({\n\
              \      assistant_id: \"asst_123\",\n      thread: {\n        messages:\
              \ [\n          { role: \"user\", content: \"Hello\" },\n        ],\n\
              \      },\n      stream: true\n  });\n\n  for await (const event of\
              \ stream) {\n    console.log(event);\n  }\n}\n\nmain();\n"
          response: 'event: thread.created

            data: {"id":"thread_123","object":"thread","created_at":1710348075,"metadata":{}}


            event: thread.run.created

            data: {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"tool_resources":{},"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


            event: thread.run.queued

            data: {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"tool_resources":{},"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


            event: thread.run.in_progress

            data: {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"tool_resources":{},"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


            event: thread.run.step.created

            data: {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


            event: thread.run.step.in_progress

            data: {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


            event: thread.message.created

            data: {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],
            "metadata":{}}


            event: thread.message.in_progress

            data: {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],
            "metadata":{}}


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"Hello","annotations":[]}}]}}


            ...


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
            today"}}]}}


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"?"}}]}}


            event: thread.message.completed

            data: {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710348077,"role":"assistant","content":[{"type":"text","text":{"value":"Hello!
            How can I assist you today?","annotations":[]}}], "metadata":{}}


            event: thread.run.step.completed

            data: {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710348077,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31}}


            event: thread.run.completed

            {"id":"run_123","object":"thread.run","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1713226836,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1713226837,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":345,"completion_tokens":11,"total_tokens":356},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


            event: done

            data: [DONE]

            '
        - title: Streaming with Functions
          request:
            curl: "curl https://api.openai.com/v1/threads/runs \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
              \ \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d '{\n    \"assistant_id\"\
              : \"asst_abc123\",\n    \"thread\": {\n      \"messages\": [\n     \
              \   {\"role\": \"user\", \"content\": \"What is the weather like in\
              \ San Francisco?\"}\n      ]\n    },\n    \"tools\": [\n      {\n  \
              \      \"type\": \"function\",\n        \"function\": {\n          \"\
              name\": \"get_current_weather\",\n          \"description\": \"Get the\
              \ current weather in a given location\",\n          \"parameters\":\
              \ {\n            \"type\": \"object\",\n            \"properties\":\
              \ {\n              \"location\": {\n                \"type\": \"string\"\
              ,\n                \"description\": \"The city and state, e.g. San Francisco,\
              \ CA\"\n              },\n              \"unit\": {\n              \
              \  \"type\": \"string\",\n                \"enum\": [\"celsius\", \"\
              fahrenheit\"]\n              }\n            },\n            \"required\"\
              : [\"location\"]\n          }\n        }\n      }\n    ],\n    \"stream\"\
              : true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ntools = [\n \
              \ {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\"\
              : \"get_current_weather\",\n      \"description\": \"Get the current\
              \ weather in a given location\",\n      \"parameters\": {\n        \"\
              type\": \"object\",\n        \"properties\": {\n          \"location\"\
              : {\n            \"type\": \"string\",\n            \"description\"\
              : \"The city and state, e.g. San Francisco, CA\",\n          },\n  \
              \        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"\
              fahrenheit\"]},\n        },\n        \"required\": [\"location\"],\n\
              \      },\n    }\n  }\n]\n\nstream = client.beta.threads.create_and_run(\n\
              \  thread={\n      \"messages\": [\n        {\"role\": \"user\", \"\
              content\": \"What is the weather like in San Francisco?\"}\n      ]\n\
              \  },\n  assistant_id=\"asst_abc123\",\n  tools=tools,\n  stream=True\n\
              )\n\nfor event in stream:\n  print(event)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nconst tools = [\n    {\n      \"type\": \"function\",\n      \"function\"\
              : {\n        \"name\": \"get_current_weather\",\n        \"description\"\
              : \"Get the current weather in a given location\",\n        \"parameters\"\
              : {\n          \"type\": \"object\",\n          \"properties\": {\n\
              \            \"location\": {\n              \"type\": \"string\",\n\
              \              \"description\": \"The city and state, e.g. San Francisco,\
              \ CA\",\n            },\n            \"unit\": {\"type\": \"string\"\
              , \"enum\": [\"celsius\", \"fahrenheit\"]},\n          },\n        \
              \  \"required\": [\"location\"],\n        },\n      }\n    }\n];\n\n\
              async function main() {\n  const stream = await openai.beta.threads.createAndRun({\n\
              \    assistant_id: \"asst_123\",\n    thread: {\n      messages: [\n\
              \        { role: \"user\", content: \"What is the weather like in San\
              \ Francisco?\" },\n      ],\n    },\n    tools: tools,\n    stream:\
              \ true\n  });\n\n  for await (const event of stream) {\n    console.log(event);\n\
              \  }\n}\n\nmain();\n"
          response: 'event: thread.created

            data: {"id":"thread_123","object":"thread","created_at":1710351818,"metadata":{}}


            event: thread.run.created

            data: {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
            the current weather in a given location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
            city and state, e.g. San Francisco, CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.queued

            data: {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
            the current weather in a given location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
            city and state, e.g. San Francisco, CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.in_progress

            data: {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710351818,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
            the current weather in a given location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
            city and state, e.g. San Francisco, CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.step.created

            data: {"id":"step_001","object":"thread.run.step","created_at":1710351819,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"tool_calls","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710352418,"failed_at":null,"last_error":null,"step_details":{"type":"tool_calls","tool_calls":[]},"usage":null}


            event: thread.run.step.in_progress

            data: {"id":"step_001","object":"thread.run.step","created_at":1710351819,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"tool_calls","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710352418,"failed_at":null,"last_error":null,"step_details":{"type":"tool_calls","tool_calls":[]},"usage":null}


            event: thread.run.step.delta

            data: {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"id":"call_XXNp8YGaFrjrSjgqxtC8JJ1B","type":"function","function":{"name":"get_current_weather","arguments":"","output":null}}]}}}


            event: thread.run.step.delta

            data: {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"{\""}}]}}}


            event: thread.run.step.delta

            data: {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"location"}}]}}}


            ...


            event: thread.run.step.delta

            data: {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"ahrenheit"}}]}}}


            event: thread.run.step.delta

            data: {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"\"}"}}]}}}


            event: thread.run.requires_action

            data: {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"requires_action","started_at":1710351818,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":{"type":"submit_tool_outputs","submit_tool_outputs":{"tool_calls":[{"id":"call_XXNp8YGaFrjrSjgqxtC8JJ1B","type":"function","function":{"name":"get_current_weather","arguments":"{\"location\":\"San
            Francisco, CA\",\"unit\":\"fahrenheit\"}"}}]}},"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
            the current weather in a given location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
            city and state, e.g. San Francisco, CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":345,"completion_tokens":11,"total_tokens":356},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: done

            data: [DONE]

            '
  /threads/{thread_id}/runs:
    get:
      operationId: listRuns
      tags:
      - Assistants
      summary: Returns a list of runs belonging to a thread.
      parameters:
      - name: thread_id
        in: path
        required: true
        schema:
          type: string
        description: The ID of the thread the run belongs to.
      - name: limit
        in: query
        description: 'A limit on the number of objects to be returned. Limit can range
          between 1 and 100, and the default is 20.

          '
        required: false
        schema:
          type: integer
          default: 20
      - name: order
        in: query
        description: 'Sort order by the `created_at` timestamp of the objects. `asc`
          for ascending order and `desc` for descending order.

          '
        schema:
          type: string
          default: desc
          enum:
          - asc
          - desc
      - name: after
        in: query
        description: 'A cursor for use in pagination. `after` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, ending with obj_foo, your subsequent call can include
          after=obj_foo in order to fetch the next page of the list.

          '
        schema:
          type: string
      - name: before
        in: query
        description: 'A cursor for use in pagination. `before` is an object ID that
          defines your place in the list. For instance, if you make a list request
          and receive 100 objects, starting with obj_foo, your subsequent call can
          include before=obj_foo in order to fetch the previous page of the list.

          '
        schema:
          type: string
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListRunsResponse'
      x-oaiMeta:
        name: List runs
        group: threads
        beta: true
        returns: A list of [run](/docs/api-reference/runs/object) objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_abc123/runs \\\n\
              \  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\"\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nruns = client.beta.threads.runs.list(\n\
              \  \"thread_abc123\"\n)\n\nprint(runs)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const runs = await openai.beta.threads.runs.list(\n\
              \    \"thread_abc123\"\n  );\n\n  console.log(runs);\n}\n\nmain();\n"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"run_abc123\",\n      \"object\": \"thread.run\",\n      \"created_at\"\
            : 1699075072,\n      \"assistant_id\": \"asst_abc123\",\n      \"thread_id\"\
            : \"thread_abc123\",\n      \"status\": \"completed\",\n      \"started_at\"\
            : 1699075072,\n      \"expires_at\": null,\n      \"cancelled_at\": null,\n\
            \      \"failed_at\": null,\n      \"completed_at\": 1699075073,\n   \
            \   \"last_error\": null,\n      \"model\": \"gpt-4o\",\n      \"instructions\"\
            : null,\n      \"incomplete_details\": null,\n      \"tools\": [\n   \
            \     {\n          \"type\": \"code_interpreter\"\n        }\n      ],\n\
            \      \"tool_resources\": {\n        \"code_interpreter\": {\n      \
            \    \"file_ids\": [\n            \"file-abc123\",\n            \"file-abc456\"\
            \n          ]\n        }\n      },\n      \"metadata\": {},\n      \"\
            usage\": {\n        \"prompt_tokens\": 123,\n        \"completion_tokens\"\
            : 456,\n        \"total_tokens\": 579\n      },\n      \"temperature\"\
            : 1.0,\n      \"top_p\": 1.0,\n      \"max_prompt_tokens\": 1000,\n  \
            \    \"max_completion_tokens\": 1000,\n      \"truncation_strategy\":\
            \ {\n        \"type\": \"auto\",\n        \"last_messages\": null\n  \
            \    },\n      \"response_format\": \"auto\",\n      \"tool_choice\":\
            \ \"auto\",\n      \"parallel_tool_calls\": true\n    },\n    {\n    \
            \  \"id\": \"run_abc456\",\n      \"object\": \"thread.run\",\n      \"\
            created_at\": 1699063290,\n      \"assistant_id\": \"asst_abc123\",\n\
            \      \"thread_id\": \"thread_abc123\",\n      \"status\": \"completed\"\
            ,\n      \"started_at\": 1699063290,\n      \"expires_at\": null,\n  \
            \    \"cancelled_at\": null,\n      \"failed_at\": null,\n      \"completed_at\"\
            : 1699063291,\n      \"last_error\": null,\n      \"model\": \"gpt-4o\"\
            ,\n      \"instructions\": null,\n      \"incomplete_details\": null,\n\
            \      \"tools\": [\n        {\n          \"type\": \"code_interpreter\"\
            \n        }\n      ],\n      \"tool_resources\": {\n        \"code_interpreter\"\
            : {\n          \"file_ids\": [\n            \"file-abc123\",\n       \
            \     \"file-abc456\"\n          ]\n        }\n      },\n      \"metadata\"\
            : {},\n      \"usage\": {\n        \"prompt_tokens\": 123,\n        \"\
            completion_tokens\": 456,\n        \"total_tokens\": 579\n      },\n \
            \     \"temperature\": 1.0,\n      \"top_p\": 1.0,\n      \"max_prompt_tokens\"\
            : 1000,\n      \"max_completion_tokens\": 1000,\n      \"truncation_strategy\"\
            : {\n        \"type\": \"auto\",\n        \"last_messages\": null\n  \
            \    },\n      \"response_format\": \"auto\",\n      \"tool_choice\":\
            \ \"auto\",\n      \"parallel_tool_calls\": true\n    }\n  ],\n  \"first_id\"\
            : \"run_abc123\",\n  \"last_id\": \"run_abc456\",\n  \"has_more\": false\n\
            }\n"
    post:
      operationId: createRun
      tags:
      - Assistants
      summary: Create a run.
      parameters:
      - in: path
        name: thread_id
        required: true
        schema:
          type: string
        description: The ID of the thread to run.
      - name: include[]
        in: query
        description: 'A list of additional fields to include in the response. Currently
          the only supported value is `step_details.tool_calls[*].file_search.results[*].content`
          to fetch the file search result content.


          See the [file search tool documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
          for more information.

          '
        schema:
          type: array
          items:
            type: string
            enum:
            - step_details.tool_calls[*].file_search.results[*].content
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateRunRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunObject'
      x-oaiMeta:
        name: Create run
        group: threads
        beta: true
        returns: A [run](/docs/api-reference/runs/object) object.
        examples:
        - title: Default
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_abc123/runs \\\n\
              \  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d\
              \ '{\n    \"assistant_id\": \"asst_abc123\"\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nrun = client.beta.threads.runs.create(\n\
              \  thread_id=\"thread_abc123\",\n  assistant_id=\"asst_abc123\"\n)\n\
              \nprint(run)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const run = await openai.beta.threads.runs.create(\n\
              \    \"thread_abc123\",\n    { assistant_id: \"asst_abc123\" }\n  );\n\
              \n  console.log(run);\n}\n\nmain();\n"
          response: "{\n  \"id\": \"run_abc123\",\n  \"object\": \"thread.run\",\n\
            \  \"created_at\": 1699063290,\n  \"assistant_id\": \"asst_abc123\",\n\
            \  \"thread_id\": \"thread_abc123\",\n  \"status\": \"queued\",\n  \"\
            started_at\": 1699063290,\n  \"expires_at\": null,\n  \"cancelled_at\"\
            : null,\n  \"failed_at\": null,\n  \"completed_at\": 1699063291,\n  \"\
            last_error\": null,\n  \"model\": \"gpt-4o\",\n  \"instructions\": null,\n\
            \  \"incomplete_details\": null,\n  \"tools\": [\n    {\n      \"type\"\
            : \"code_interpreter\"\n    }\n  ],\n  \"metadata\": {},\n  \"usage\"\
            : null,\n  \"temperature\": 1.0,\n  \"top_p\": 1.0,\n  \"max_prompt_tokens\"\
            : 1000,\n  \"max_completion_tokens\": 1000,\n  \"truncation_strategy\"\
            : {\n    \"type\": \"auto\",\n    \"last_messages\": null\n  },\n  \"\
            response_format\": \"auto\",\n  \"tool_choice\": \"auto\",\n  \"parallel_tool_calls\"\
            : true\n}\n"
        - title: Streaming
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_123/runs \\\n  -H\
              \ \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d\
              \ '{\n    \"assistant_id\": \"asst_123\",\n    \"stream\": true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nstream = client.beta.threads.runs.create(\n\
              \  thread_id=\"thread_123\",\n  assistant_id=\"asst_123\",\n  stream=True\n\
              )\n\nfor event in stream:\n  print(event)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const stream = await openai.beta.threads.runs.create(\n\
              \    \"thread_123\",\n    { assistant_id: \"asst_123\", stream: true\
              \ }\n  );\n\n  for await (const event of stream) {\n    console.log(event);\n\
              \  }\n}\n\nmain();\n"
          response: 'event: thread.run.created

            data: {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710331240,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.queued

            data: {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710331240,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.in_progress

            data: {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710330641,"expires_at":1710331240,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.step.created

            data: {"id":"step_001","object":"thread.run.step","created_at":1710330641,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710331240,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


            event: thread.run.step.in_progress

            data: {"id":"step_001","object":"thread.run.step","created_at":1710330641,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710331240,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


            event: thread.message.created

            data: {"id":"msg_001","object":"thread.message","created_at":1710330641,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


            event: thread.message.in_progress

            data: {"id":"msg_001","object":"thread.message","created_at":1710330641,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"Hello","annotations":[]}}]}}


            ...


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
            today"}}]}}


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"?"}}]}}


            event: thread.message.completed

            data: {"id":"msg_001","object":"thread.message","created_at":1710330641,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710330642,"role":"assistant","content":[{"type":"text","text":{"value":"Hello!
            How can I assist you today?","annotations":[]}}],"metadata":{}}


            event: thread.run.step.completed

            data: {"id":"step_001","object":"thread.run.step","created_at":1710330641,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710330642,"expires_at":1710331240,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31}}


            event: thread.run.completed

            data: {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1710330641,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1710330642,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: done

            data: [DONE]

            '
        - title: Streaming with Functions
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_abc123/runs \\\n\
              \  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d\
              \ '{\n    \"assistant_id\": \"asst_abc123\",\n    \"tools\": [\n   \
              \   {\n        \"type\": \"function\",\n        \"function\": {\n  \
              \        \"name\": \"get_current_weather\",\n          \"description\"\
              : \"Get the current weather in a given location\",\n          \"parameters\"\
              : {\n            \"type\": \"object\",\n            \"properties\":\
              \ {\n              \"location\": {\n                \"type\": \"string\"\
              ,\n                \"description\": \"The city and state, e.g. San Francisco,\
              \ CA\"\n              },\n              \"unit\": {\n              \
              \  \"type\": \"string\",\n                \"enum\": [\"celsius\", \"\
              fahrenheit\"]\n              }\n            },\n            \"required\"\
              : [\"location\"]\n          }\n        }\n      }\n    ],\n    \"stream\"\
              : true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ntools = [\n \
              \ {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\"\
              : \"get_current_weather\",\n      \"description\": \"Get the current\
              \ weather in a given location\",\n      \"parameters\": {\n        \"\
              type\": \"object\",\n        \"properties\": {\n          \"location\"\
              : {\n            \"type\": \"string\",\n            \"description\"\
              : \"The city and state, e.g. San Francisco, CA\",\n          },\n  \
              \        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"\
              fahrenheit\"]},\n        },\n        \"required\": [\"location\"],\n\
              \      },\n    }\n  }\n]\n\nstream = client.beta.threads.runs.create(\n\
              \  thread_id=\"thread_abc123\",\n  assistant_id=\"asst_abc123\",\n \
              \ tools=tools,\n  stream=True\n)\n\nfor event in stream:\n  print(event)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nconst tools = [\n    {\n      \"type\": \"function\",\n      \"function\"\
              : {\n        \"name\": \"get_current_weather\",\n        \"description\"\
              : \"Get the current weather in a given location\",\n        \"parameters\"\
              : {\n          \"type\": \"object\",\n          \"properties\": {\n\
              \            \"location\": {\n              \"type\": \"string\",\n\
              \              \"description\": \"The city and state, e.g. San Francisco,\
              \ CA\",\n            },\n            \"unit\": {\"type\": \"string\"\
              , \"enum\": [\"celsius\", \"fahrenheit\"]},\n          },\n        \
              \  \"required\": [\"location\"],\n        },\n      }\n    }\n];\n\n\
              async function main() {\n  const stream = await openai.beta.threads.runs.create(\n\
              \    \"thread_abc123\",\n    {\n      assistant_id: \"asst_abc123\"\
              ,\n      tools: tools,\n      stream: true\n    }\n  );\n\n  for await\
              \ (const event of stream) {\n    console.log(event);\n  }\n}\n\nmain();\n"
          response: 'event: thread.run.created

            data: {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.queued

            data: {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.in_progress

            data: {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710348075,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.step.created

            data: {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


            event: thread.run.step.in_progress

            data: {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


            event: thread.message.created

            data: {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


            event: thread.message.in_progress

            data: {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"Hello","annotations":[]}}]}}


            ...


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
            today"}}]}}


            event: thread.message.delta

            data: {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"?"}}]}}


            event: thread.message.completed

            data: {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710348077,"role":"assistant","content":[{"type":"text","text":{"value":"Hello!
            How can I assist you today?","annotations":[]}}],"metadata":{}}


            event: thread.run.step.completed

            data: {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710348077,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31}}


            event: thread.run.completed

            data: {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1710348075,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1710348077,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: done

            data: [DONE]

            '
  /threads/{thread_id}/runs/{run_id}:
    get:
      operationId: getRun
      tags:
      - Assistants
      summary: Retrieves a run.
      parameters:
      - in: path
        name: thread_id
        required: true
        schema:
          type: string
        description: The ID of the [thread](/docs/api-reference/threads) that was
          run.
      - in: path
        name: run_id
        required: true
        schema:
          type: string
        description: The ID of the run to retrieve.
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunObject'
      x-oaiMeta:
        name: Retrieve run
        group: threads
        beta: true
        returns: The [run](/docs/api-reference/runs/object) object matching the specified
          ID.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_abc123/runs/run_abc123\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"OpenAI-Beta:\
              \ assistants=v2\"\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nrun = client.beta.threads.runs.retrieve(\n\
              \  thread_id=\"thread_abc123\",\n  run_id=\"run_abc123\"\n)\n\nprint(run)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const run = await openai.beta.threads.runs.retrieve(\n\
              \    \"thread_abc123\",\n    \"run_abc123\"\n  );\n\n  console.log(run);\n\
              }\n\nmain();\n"
          response: "{\n  \"id\": \"run_abc123\",\n  \"object\": \"thread.run\",\n\
            \  \"created_at\": 1699075072,\n  \"assistant_id\": \"asst_abc123\",\n\
            \  \"thread_id\": \"thread_abc123\",\n  \"status\": \"completed\",\n \
            \ \"started_at\": 1699075072,\n  \"expires_at\": null,\n  \"cancelled_at\"\
            : null,\n  \"failed_at\": null,\n  \"completed_at\": 1699075073,\n  \"\
            last_error\": null,\n  \"model\": \"gpt-4o\",\n  \"instructions\": null,\n\
            \  \"incomplete_details\": null,\n  \"tools\": [\n    {\n      \"type\"\
            : \"code_interpreter\"\n    }\n  ],\n  \"metadata\": {},\n  \"usage\"\
            : {\n    \"prompt_tokens\": 123,\n    \"completion_tokens\": 456,\n  \
            \  \"total_tokens\": 579\n  },\n  \"temperature\": 1.0,\n  \"top_p\":\
            \ 1.0,\n  \"max_prompt_tokens\": 1000,\n  \"max_completion_tokens\": 1000,\n\
            \  \"truncation_strategy\": {\n    \"type\": \"auto\",\n    \"last_messages\"\
            : null\n  },\n  \"response_format\": \"auto\",\n  \"tool_choice\": \"\
            auto\",\n  \"parallel_tool_calls\": true\n}\n"
  /threads/{thread_id}/runs/{run_id}/submit_tool_outputs:
    post:
      operationId: submitToolOuputsToRun
      tags:
      - Assistants
      summary: 'When a run has the `status: "requires_action"` and `required_action.type`
        is `submit_tool_outputs`, this endpoint can be used to submit the outputs
        from the tool calls once they''re all completed. All outputs must be submitted
        in a single request.

        '
      parameters:
      - in: path
        name: thread_id
        required: true
        schema:
          type: string
        description: The ID of the [thread](/docs/api-reference/threads) to which
          this run belongs.
      - in: path
        name: run_id
        required: true
        schema:
          type: string
        description: The ID of the run that requires the tool output submission.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/SubmitToolOutputsRunRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RunObject'
      x-oaiMeta:
        name: Submit tool outputs to run
        group: threads
        beta: true
        returns: The modified [run](/docs/api-reference/runs/object) object matching
          the specified ID.
        examples:
        - title: Default
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_123/runs/run_123/submit_tool_outputs\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d\
              \ '{\n    \"tool_outputs\": [\n      {\n        \"tool_call_id\": \"\
              call_001\",\n        \"output\": \"70 degrees and sunny.\"\n      }\n\
              \    ]\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nrun = client.beta.threads.runs.submit_tool_outputs(\n\
              \  thread_id=\"thread_123\",\n  run_id=\"run_123\",\n  tool_outputs=[\n\
              \    {\n      \"tool_call_id\": \"call_001\",\n      \"output\": \"\
              70 degrees and sunny.\"\n    }\n  ]\n)\n\nprint(run)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const run = await openai.beta.threads.runs.submitToolOutputs(\n\
              \    \"thread_123\",\n    \"run_123\",\n    {\n      tool_outputs: [\n\
              \        {\n          tool_call_id: \"call_001\",\n          output:\
              \ \"70 degrees and sunny.\",\n        },\n      ],\n    }\n  );\n\n\
              \  console.log(run);\n}\n\nmain();\n"
          response: "{\n  \"id\": \"run_123\",\n  \"object\": \"thread.run\",\n  \"\
            created_at\": 1699075592,\n  \"assistant_id\": \"asst_123\",\n  \"thread_id\"\
            : \"thread_123\",\n  \"status\": \"queued\",\n  \"started_at\": 1699075592,\n\
            \  \"expires_at\": 1699076192,\n  \"cancelled_at\": null,\n  \"failed_at\"\
            : null,\n  \"completed_at\": null,\n  \"last_error\": null,\n  \"model\"\
            : \"gpt-4o\",\n  \"instructions\": null,\n  \"tools\": [\n    {\n    \
            \  \"type\": \"function\",\n      \"function\": {\n        \"name\": \"\
            get_current_weather\",\n        \"description\": \"Get the current weather\
            \ in a given location\",\n        \"parameters\": {\n          \"type\"\
            : \"object\",\n          \"properties\": {\n            \"location\":\
            \ {\n              \"type\": \"string\",\n              \"description\"\
            : \"The city and state, e.g. San Francisco, CA\"\n            },\n   \
            \         \"unit\": {\n              \"type\": \"string\",\n         \
            \     \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n       \
            \   },\n          \"required\": [\"location\"]\n        }\n      }\n \
            \   }\n  ],\n  \"metadata\": {},\n  \"usage\": null,\n  \"temperature\"\
            : 1.0,\n  \"top_p\": 1.0,\n  \"max_prompt_tokens\": 1000,\n  \"max_completion_tokens\"\
            : 1000,\n  \"truncation_strategy\": {\n    \"type\": \"auto\",\n    \"\
            last_messages\": null\n  },\n  \"response_format\": \"auto\",\n  \"tool_choice\"\
            : \"auto\",\n  \"parallel_tool_calls\": true\n}\n"
        - title: Streaming
          request:
            curl: "curl https://api.openai.com/v1/threads/thread_123/runs/run_123/submit_tool_outputs\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"OpenAI-Beta: assistants=v2\" \\\n  -d\
              \ '{\n    \"tool_outputs\": [\n      {\n        \"tool_call_id\": \"\
              call_001\",\n        \"output\": \"70 degrees and sunny.\"\n      }\n\
              \    ],\n    \"stream\": true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nstream = client.beta.threads.runs.submit_tool_outputs(\n\
              \  thread_id=\"thread_123\",\n  run_id=\"run_123\",\n  tool_outputs=[\n\
              \    {\n      \"tool_call_id\": \"call_001\",\n      \"output\": \"\
              70 degrees and sunny.\"\n    }\n  ],\n  stream=True\n)\n\nfor event\
              \ in stream:\n  print(event)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const stream = await openai.beta.threads.runs.submitToolOutputs(\n\
              \    \"thread_123\",\n    \"run_123\",\n    {\n      tool_outputs: [\n\
              \        {\n          tool_call_id: \"call_001\",\n          output:\
              \ \"70 degrees and sunny.\",\n        },\n      ],\n    }\n  );\n\n\
              \  for await (const event of stream) {\n    console.log(event);\n  }\n\
              }\n\nmain();\n"
          response: 'event: thread.run.step.completed

            data: {"id":"step_001","object":"thread.run.step","created_at":1710352449,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"tool_calls","status":"completed","cancelled_at":null,"completed_at":1710352475,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"tool_calls","tool_calls":[{"id":"call_iWr0kQ2EaYMaxNdl0v3KYkx7","type":"function","function":{"name":"get_current_weather","arguments":"{\"location\":\"San
            Francisco, CA\",\"unit\":\"fahrenheit\"}","output":"70 degrees and sunny."}}]},"usage":{"prompt_tokens":291,"completion_tokens":24,"total_tokens":315}}


            event: thread.run.queued

            data: {"id":"run_123","object":"thread.run","created_at":1710352447,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":1710352448,"expires_at":1710353047,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
            the current weather in a given location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
            city and state, e.g. San Francisco, CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.in_progress

            data: {"id":"run_123","object":"thread.run","created_at":1710352447,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710352475,"expires_at":1710353047,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
            the current weather in a given location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
            city and state, e.g. San Francisco, CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: thread.run.step.created

            data: {"id":"step_002","object":"thread.run.step","created_at":1710352476,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_002"}},"usage":null}


            event: thread.run.step.in_progress

            data: {"id":"step_002","object":"thread.run.step","created_at":1710352476,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_002"}},"usage":null}


            event: thread.message.created

            data: {"id":"msg_002","object":"thread.message","created_at":1710352476,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


            event: thread.message.in_progress

            data: {"id":"msg_002","object":"thread.message","created_at":1710352476,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


            event: thread.message.delta

            data: {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"The","annotations":[]}}]}}


            event: thread.message.delta

            data: {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
            current"}}]}}


            event: thread.message.delta

            data: {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
            weather"}}]}}


            ...


            event: thread.message.delta

            data: {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
            sunny"}}]}}


            event: thread.message.delta

            data: {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"."}}]}}


            event: thread.message.completed

            data: {"id":"msg_002","object":"thread.message","created_at":1710352476,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710352477,"role":"assistant","content":[{"type":"text","text":{"value":"The
            current weather in San Francisco, CA is 70 degrees Fahrenheit and sunny.","annotations":[]}}],"metadata":{}}


            event: thread.run.step.completed

            data: {"id":"step_002","object":"thread.run.step","created_at":1710352476,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710352477,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_002"}},"usage":{"prompt_tokens":329,"completion_tokens":18,"total_tokens":347}}


            event: thread.run.completed

            data: {"id":"run_123","object":"thread.run","created_at":1710352447,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1710352475,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1710352477,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
            the current weather in a given location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
            city and state, e.g. San Francisco, CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


            event: done

            data: [DONE]

            '
components:
  schemas:
    AssistantObject:
      type: object
      title: Assistant
      description: Represents an `assistant` that can call the model and use tools.
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `assistant`.
          type: string
          enum:
          - assistant
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the assistant was
            created.
          type: integer
        name:
          description: 'The name of the assistant. The maximum length is 256 characters.

            '
          type: string
          maxLength: 256
          nullable: true
        description:
          description: 'The description of the assistant. The maximum length is 512
            characters.

            '
          type: string
          maxLength: 512
          nullable: true
        model:
          description: 'ID of the model to use. You can use the [List models](/docs/api-reference/models/list)
            API to see all of your available models, or see our [Model overview](/docs/models)
            for descriptions of them.

            '
          type: string
        instructions:
          description: 'The system instructions that the assistant uses. The maximum
            length is 256,000 characters.

            '
          type: string
          maxLength: 256000
          nullable: true
        tools:
          description: 'A list of tool enabled on the assistant. There can be a maximum
            of 128 tools per assistant. Tools can be of types `code_interpreter`,
            `file_search`, or `function`.

            '
          default: []
          type: array
          maxItems: 128
          items:
            oneOf:
            - $ref: '#/components/schemas/AssistantToolsCode'
            - $ref: '#/components/schemas/AssistantToolsFileSearch'
            - $ref: '#/components/schemas/AssistantToolsFunction'
            x-oaiExpandable: true
        tool_resources:
          type: object
          description: 'A set of resources that are used by the assistant''s tools.
            The resources are specific to the type of tool. For example, the `code_interpreter`
            tool requires a list of file IDs, while the `file_search` tool requires
            a list of vector store IDs.

            '
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: 'A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter`` tool. There can be a maximum
                    of 20 files associated with the tool.

                    '
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: 'The ID of the [vector store](/docs/api-reference/vector-stores/object)
                    attached to this assistant. There can be a maximum of 1 vector
                    store attached to the assistant.

                    '
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
        temperature:
          description: 'What sampling temperature to use, between 0 and 2. Higher
            values like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.

            '
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: 'An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with top_p
            probability mass. So 0.1 means only the tokens comprising the top 10%
            probability mass are considered.


            We generally recommend altering this or temperature but not both.

            '
        response_format:
          $ref: '#/components/schemas/AssistantsApiResponseFormatOption'
          nullable: true
      required:
      - id
      - object
      - created_at
      - name
      - description
      - model
      - instructions
      - tools
      - metadata
      x-oaiMeta:
        name: The assistant object
        beta: true
        example: "{\n  \"id\": \"asst_abc123\",\n  \"object\": \"assistant\",\n  \"\
          created_at\": 1698984975,\n  \"name\": \"Math Tutor\",\n  \"description\"\
          : null,\n  \"model\": \"gpt-4o\",\n  \"instructions\": \"You are a personal\
          \ math tutor. When asked a question, write and run Python code to answer\
          \ the question.\",\n  \"tools\": [\n    {\n      \"type\": \"code_interpreter\"\
          \n    }\n  ],\n  \"metadata\": {},\n  \"top_p\": 1.0,\n  \"temperature\"\
          : 1.0,\n  \"response_format\": \"auto\"\n}\n"
    AssistantSupportedModels:
      type: string
      enum:
      - o3-mini
      - o3-mini-2025-01-31
      - o1
      - o1-2024-12-17
      - gpt-4o
      - gpt-4o-2024-11-20
      - gpt-4o-2024-08-06
      - gpt-4o-2024-05-13
      - gpt-4o-mini
      - gpt-4o-mini-2024-07-18
      - gpt-4.5-preview
      - gpt-4.5-preview-2025-02-27
      - gpt-4-turbo
      - gpt-4-turbo-2024-04-09
      - gpt-4-0125-preview
      - gpt-4-turbo-preview
      - gpt-4-1106-preview
      - gpt-4-vision-preview
      - gpt-4
      - gpt-4-0314
      - gpt-4-0613
      - gpt-4-32k
      - gpt-4-32k-0314
      - gpt-4-32k-0613
      - gpt-3.5-turbo
      - gpt-3.5-turbo-16k
      - gpt-3.5-turbo-0613
      - gpt-3.5-turbo-1106
      - gpt-3.5-turbo-0125
      - gpt-3.5-turbo-16k-0613
    AssistantToolsCode:
      type: object
      title: Code interpreter tool
      properties:
        type:
          type: string
          description: 'The type of tool being defined: `code_interpreter`'
          enum:
          - code_interpreter
          x-stainless-const: true
      required:
      - type
    AssistantToolsFileSearch:
      type: object
      title: FileSearch tool
      properties:
        type:
          type: string
          description: 'The type of tool being defined: `file_search`'
          enum:
          - file_search
          x-stainless-const: true
        file_search:
          type: object
          description: Overrides for the file search tool.
          properties:
            max_num_results:
              type: integer
              minimum: 1
              maximum: 50
              description: 'The maximum number of results the file search tool should
                output. The default is 20 for `gpt-4*` models and 5 for `gpt-3.5-turbo`.
                This number should be between 1 and 50 inclusive.


                Note that the file search tool may output fewer than `max_num_results`
                results. See the [file search tool documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
                for more information.

                '
            ranking_options:
              $ref: '#/components/schemas/FileSearchRankingOptions'
      required:
      - type
    AssistantToolsFileSearchTypeOnly:
      type: object
      title: FileSearch tool
      properties:
        type:
          type: string
          description: 'The type of tool being defined: `file_search`'
          enum:
          - file_search
          x-stainless-const: true
      required:
      - type
    AssistantToolsFunction:
      type: object
      title: Function tool
      properties:
        type:
          type: string
          description: 'The type of tool being defined: `function`'
          enum:
          - function
          x-stainless-const: true
        function:
          $ref: '#/components/schemas/FunctionObject'
      required:
      - type
      - function
    AssistantsApiResponseFormatOption:
      description: 'Specifies the format that the model must output. Compatible with
        [GPT-4o](/docs/models#gpt-4o), [GPT-4 Turbo](/docs/models#gpt-4-turbo-and-gpt-4),
        and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.


        Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        Outputs which ensures the model will match your supplied JSON schema. Learn
        more in the [Structured Outputs guide](/docs/guides/structured-outputs).


        Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the
        message the model generates is valid JSON.


        **Important:** when using JSON mode, you **must** also instruct the model
        to produce JSON yourself via a system or user message. Without this, the model
        may generate an unending stream of whitespace until the generation reaches
        the token limit, resulting in a long-running and seemingly "stuck" request.
        Also note that the message content may be partially cut off if `finish_reason="length"`,
        which indicates the generation exceeded `max_tokens` or the conversation exceeded
        the max context length.

        '
      oneOf:
      - type: string
        description: '`auto` is the default value

          '
        enum:
        - auto
        x-stainless-const: true
      - $ref: '#/components/schemas/ResponseFormatText'
      - $ref: '#/components/schemas/ResponseFormatJsonObject'
      - $ref: '#/components/schemas/ResponseFormatJsonSchema'
      x-oaiExpandable: true
    AssistantsApiToolChoiceOption:
      description: 'Controls which (if any) tool is called by the model.

        `none` means the model will not call any tools and instead generates a message.

        `auto` is the default value and means the model can pick between generating
        a message or calling one or more tools.

        `required` means the model must call one or more tools before responding to
        the user.

        Specifying a particular tool like `{"type": "file_search"}` or `{"type": "function",
        "function": {"name": "my_function"}}` forces the model to call that tool.

        '
      oneOf:
      - type: string
        description: '`none` means the model will not call any tools and instead generates
          a message. `auto` means the model can pick between generating a message
          or calling one or more tools. `required` means the model must call one or
          more tools before responding to the user.

          '
        enum:
        - none
        - auto
        - required
      - $ref: '#/components/schemas/AssistantsNamedToolChoice'
      x-oaiExpandable: true
    AssistantsNamedToolChoice:
      type: object
      description: Specifies a tool the model should use. Use to force the model to
        call a specific tool.
      properties:
        type:
          type: string
          enum:
          - function
          - code_interpreter
          - file_search
          description: The type of the tool. If type is `function`, the function name
            must be set
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
          required:
          - name
      required:
      - type
    AutoChunkingStrategyRequestParam:
      type: object
      title: Auto Chunking Strategy
      description: The default strategy. This strategy currently uses a `max_chunk_size_tokens`
        of `800` and `chunk_overlap_tokens` of `400`.
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `auto`.
          enum:
          - auto
          x-stainless-const: true
      required:
      - type
    ChatCompletionFunctionCallOption:
      type: object
      description: 'Specifying a particular function via `{"name": "my_function"}`
        forces the model to call that function.

        '
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
      - name
    ChatCompletionFunctions:
      type: object
      deprecated: true
      properties:
        description:
          type: string
          description: A description of what the function does, used by the model
            to choose when and how to call the function.
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9,
            or contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: '#/components/schemas/FunctionParameters'
      required:
      - name
    ChatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          description: The function that the model called.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
          required:
          - name
          - arguments
      required:
      - id
      - type
      - function
    ChatCompletionMessageToolCallChunk:
      type: object
      properties:
        index:
          type: integer
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
      required:
      - index
    ChatCompletionMessageToolCalls:
      type: array
      description: The tool calls generated by the model, such as function calls.
      items:
        $ref: '#/components/schemas/ChatCompletionMessageToolCall'
    ChatCompletionNamedToolChoice:
      type: object
      description: Specifies a tool the model should use. Use to force the model to
        call a specific function.
      properties:
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
          required:
          - name
      required:
      - type
      - function
    ChatCompletionRequestAssistantMessage:
      type: object
      title: Assistant message
      description: 'Messages sent by the model in response to user messages.

        '
      properties:
        content:
          x-oaiExpandable: true
          nullable: true
          oneOf:
          - type: string
            description: The contents of the assistant message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. Can be one
              or more of type `text`, or exactly one of type `refusal`.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestAssistantMessageContentPart'
            minItems: 1
          description: 'The contents of the assistant message. Required unless `tool_calls`
            or `function_call` is specified.

            '
        refusal:
          nullable: true
          type: string
          description: The refusal message by the assistant.
        role:
          type: string
          enum:
          - assistant
          description: The role of the messages author, in this case `assistant`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
        audio:
          type: object
          nullable: true
          x-oaiExpandable: true
          description: "Data about a previous audio response from the model. \n[Learn\
            \ more](/docs/guides/audio).\n"
          required:
          - id
          properties:
            id:
              type: string
              description: 'Unique identifier for a previous audio response from the
                model.

                '
        tool_calls:
          $ref: '#/components/schemas/ChatCompletionMessageToolCalls'
        function_call:
          type: object
          deprecated: true
          description: Deprecated and replaced by `tool_calls`. The name and arguments
            of a function that should be called, as generated by the model.
          nullable: true
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
          - arguments
          - name
      required:
      - role
    ChatCompletionRequestAssistantMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartRefusal'
      x-oaiExpandable: true
    ChatCompletionRequestDeveloperMessage:
      type: object
      title: Developer message
      description: 'Developer-provided instructions that the model should follow,
        regardless of

        messages sent by the user. With o1 models and newer, `developer` messages

        replace the previous `system` messages.

        '
      properties:
        content:
          description: The contents of the developer message.
          oneOf:
          - type: string
            description: The contents of the developer message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. For developer
              messages, only type `text` is supported.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
            minItems: 1
        role:
          type: string
          enum:
          - developer
          description: The role of the messages author, in this case `developer`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
      - content
      - role
    ChatCompletionRequestFunctionMessage:
      type: object
      title: Function message
      deprecated: true
      properties:
        role:
          type: string
          enum:
          - function
          description: The role of the messages author, in this case `function`.
          x-stainless-const: true
        content:
          nullable: true
          type: string
          description: The contents of the function message.
        name:
          type: string
          description: The name of the function to call.
      required:
      - role
      - content
      - name
    ChatCompletionRequestMessage:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestDeveloperMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestSystemMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestUserMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestAssistantMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestToolMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestFunctionMessage'
      x-oaiExpandable: true
    ChatCompletionRequestMessageContentPartAudio:
      type: object
      title: Audio content part
      description: 'Learn about [audio inputs](/docs/guides/audio).

        '
      properties:
        type:
          type: string
          enum:
          - input_audio
          description: The type of the content part. Always `input_audio`.
          x-stainless-const: true
        input_audio:
          type: object
          properties:
            data:
              type: string
              description: Base64 encoded audio data.
            format:
              type: string
              enum:
              - wav
              - mp3
              description: 'The format of the encoded audio data. Currently supports
                "wav" and "mp3".

                '
          required:
          - data
          - format
      required:
      - type
      - input_audio
    ChatCompletionRequestMessageContentPartFile:
      type: object
      title: File content part
      description: 'Learn about [file inputs](/docs/guides/text) for text generation.

        '
      properties:
        type:
          type: string
          enum:
          - file
          description: The type of the content part. Always `file`.
          x-stainless-const: true
        file:
          type: object
          properties:
            file_name:
              type: string
              description: "The name of the file, used when passing the file to the\
                \ model as a \nstring.\n"
            file_data:
              type: string
              description: "The base64 encoded file data, used when passing the file\
                \ to the model \nas a string.\n"
            file_id:
              type: string
              description: 'The ID of an uploaded file to use as input.

                '
      required:
      - type
      - file
    ChatCompletionRequestMessageContentPartImage:
      type: object
      title: Image content part
      description: 'Learn about [image inputs](/docs/guides/vision).

        '
      properties:
        type:
          type: string
          enum:
          - image_url
          description: The type of the content part.
          x-stainless-const: true
        image_url:
          type: object
          properties:
            url:
              type: string
              description: Either a URL of the image or the base64 encoded image data.
              format: uri
            detail:
              type: string
              description: Specifies the detail level of the image. Learn more in
                the [Vision guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
              enum:
              - auto
              - low
              - high
              default: auto
          required:
          - url
      required:
      - type
      - image_url
    ChatCompletionRequestMessageContentPartRefusal:
      type: object
      title: Refusal content part
      properties:
        type:
          type: string
          enum:
          - refusal
          description: The type of the content part.
          x-stainless-const: true
        refusal:
          type: string
          description: The refusal message generated by the model.
      required:
      - type
      - refusal
    ChatCompletionRequestMessageContentPartText:
      type: object
      title: Text content part
      description: 'Learn about [text inputs](/docs/guides/text-generation).

        '
      properties:
        type:
          type: string
          enum:
          - text
          description: The type of the content part.
          x-stainless-const: true
        text:
          type: string
          description: The text content.
      required:
      - type
      - text
    ChatCompletionRequestSystemMessage:
      type: object
      title: System message
      description: 'Developer-provided instructions that the model should follow,
        regardless of

        messages sent by the user. With o1 models and newer, use `developer` messages

        for this purpose instead.

        '
      properties:
        content:
          description: The contents of the system message.
          oneOf:
          - type: string
            description: The contents of the system message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. For system
              messages, only type `text` is supported.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestSystemMessageContentPart'
            minItems: 1
        role:
          type: string
          enum:
          - system
          description: The role of the messages author, in this case `system`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
      - content
      - role
    ChatCompletionRequestSystemMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
      x-oaiExpandable: true
    ChatCompletionRequestToolMessage:
      type: object
      title: Tool message
      properties:
        role:
          type: string
          enum:
          - tool
          description: The role of the messages author, in this case `tool`.
          x-stainless-const: true
        content:
          oneOf:
          - type: string
            description: The contents of the tool message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. For tool messages,
              only type `text` is supported.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestToolMessageContentPart'
            minItems: 1
          description: The contents of the tool message.
        tool_call_id:
          type: string
          description: Tool call that this message is responding to.
      required:
      - role
      - content
      - tool_call_id
    ChatCompletionRequestToolMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
      x-oaiExpandable: true
    ChatCompletionRequestUserMessage:
      type: object
      title: User message
      description: 'Messages sent by an end user, containing prompts or additional
        context

        information.

        '
      properties:
        content:
          description: 'The contents of the user message.

            '
          oneOf:
          - type: string
            description: The text contents of the message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. Supported
              options differ based on the [model](/docs/models) being used to generate
              the response. Can contain text, image, or audio inputs.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestUserMessageContentPart'
            minItems: 1
          x-oaiExpandable: true
        role:
          type: string
          enum:
          - user
          description: The role of the messages author, in this case `user`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
      - content
      - role
    ChatCompletionRequestUserMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartImage'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartAudio'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartFile'
      x-oaiExpandable: true
    ChatCompletionResponseMessage:
      type: object
      description: A chat completion message generated by the model.
      properties:
        content:
          type: string
          description: The contents of the message.
          nullable: true
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
        tool_calls:
          $ref: '#/components/schemas/ChatCompletionMessageToolCalls'
        annotations:
          type: array
          description: 'Annotations for the message, when applicable, as when using
            the

            [web search tool](/docs/guides/tools-web-search?api-mode=chat).

            '
          items:
            type: object
            description: 'A URL citation when using web search.

              '
            required:
            - type
            - url_citation
            properties:
              type:
                type: string
                description: The type of the URL citation. Always `url_citation`.
                enum:
                - url_citation
                x-stainless-const: true
              url_citation:
                type: object
                description: A URL citation when using web search.
                required:
                - end_index
                - start_index
                - url
                - title
                properties:
                  end_index:
                    type: integer
                    description: The index of the last character of the URL citation
                      in the message.
                  start_index:
                    type: integer
                    description: The index of the first character of the URL citation
                      in the message.
                  url:
                    type: string
                    description: The URL of the web resource.
                  title:
                    type: string
                    description: The title of the web resource.
        role:
          type: string
          enum:
          - assistant
          description: The role of the author of this message.
          x-stainless-const: true
        function_call:
          type: object
          deprecated: true
          description: Deprecated and replaced by `tool_calls`. The name and arguments
            of a function that should be called, as generated by the model.
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
          - name
          - arguments
        audio:
          type: object
          nullable: true
          description: 'If the audio output modality is requested, this object contains
            data

            about the audio response from the model. [Learn more](/docs/guides/audio).

            '
          x-oaiExpandable: true
          required:
          - id
          - expires_at
          - data
          - transcript
          properties:
            id:
              type: string
              description: Unique identifier for this audio response.
            expires_at:
              type: integer
              description: 'The Unix timestamp (in seconds) for when this audio response
                will

                no longer be accessible on the server for use in multi-turn

                conversations.

                '
            data:
              type: string
              description: 'Base64 encoded audio bytes generated by the model, in
                the format

                specified in the request.

                '
            transcript:
              type: string
              description: Transcript of the audio generated by the model.
      required:
      - role
      - content
      - refusal
    ChatCompletionStreamOptions:
      description: 'Options for streaming response. Only set this when you set `stream:
        true`.

        '
      type: object
      nullable: true
      default: null
      properties:
        include_usage:
          type: boolean
          description: 'If set, an additional chunk will be streamed before the `data:
            [DONE]` message. The `usage` field on this chunk shows the token usage
            statistics for the entire request, and the `choices` field will always
            be an empty array. All other chunks will also include a `usage` field,
            but with a null value.

            '
    ChatCompletionStreamResponseDelta:
      type: object
      description: A chat completion delta generated by streamed model responses.
      properties:
        content:
          type: string
          description: The contents of the chunk message.
          nullable: true
        function_call:
          deprecated: true
          type: object
          description: Deprecated and replaced by `tool_calls`. The name and arguments
            of a function that should be called, as generated by the model.
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
        tool_calls:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionMessageToolCallChunk'
        role:
          type: string
          enum:
          - developer
          - system
          - user
          - assistant
          - tool
          description: The role of the author of this message.
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
    ChatCompletionTokenLogprob:
      type: object
      properties:
        token: &id001
          description: The token.
          type: string
        logprob: &id002
          description: The log probability of this token, if it is within the top
            20 most likely tokens. Otherwise, the value `-9999.0` is used to signify
            that the token is very unlikely.
          type: number
        bytes: &id003
          description: A list of integers representing the UTF-8 bytes representation
            of the token. Useful in instances where characters are represented by
            multiple tokens and their byte representations must be combined to generate
            the correct text representation. Can be `null` if there is no bytes representation
            for the token.
          type: array
          items:
            type: integer
          nullable: true
        top_logprobs:
          description: List of the most likely tokens and their log probability, at
            this token position. In rare cases, there may be fewer than the number
            of requested `top_logprobs` returned.
          type: array
          items:
            type: object
            properties:
              token: *id001
              logprob: *id002
              bytes: *id003
            required:
            - token
            - logprob
            - bytes
      required:
      - token
      - logprob
      - bytes
      - top_logprobs
    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          $ref: '#/components/schemas/FunctionObject'
      required:
      - type
      - function
    ChatCompletionToolChoiceOption:
      description: 'Controls which (if any) tool is called by the model.

        `none` means the model will not call any tool and instead generates a message.

        `auto` means the model can pick between generating a message or calling one
        or more tools.

        `required` means the model must call one or more tools.

        Specifying a particular tool via `{"type": "function", "function": {"name":
        "my_function"}}` forces the model to call that tool.


        `none` is the default when no tools are present. `auto` is the default if
        tools are present.

        '
      oneOf:
      - type: string
        description: '`none` means the model will not call any tool and instead generates
          a message. `auto` means the model can pick between generating a message
          or calling one or more tools. `required` means the model must call one or
          more tools.

          '
        enum:
        - none
        - auto
        - required
      - $ref: '#/components/schemas/ChatCompletionNamedToolChoice'
      x-oaiExpandable: true
    ChunkingStrategyRequestParam:
      type: object
      description: The chunking strategy used to chunk the file(s). If not set, will
        use the `auto` strategy.
      oneOf:
      - $ref: '#/components/schemas/AutoChunkingStrategyRequestParam'
      - $ref: '#/components/schemas/StaticChunkingStrategyRequestParam'
      x-oaiExpandable: true
    CompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      properties:
        completion_tokens:
          type: integer
          default: 0
          description: Number of tokens in the generated completion.
        prompt_tokens:
          type: integer
          default: 0
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          default: 0
          description: Total number of tokens used in the request (prompt + completion).
        completion_tokens_details:
          type: object
          description: Breakdown of tokens used in a completion.
          properties:
            accepted_prediction_tokens:
              type: integer
              default: 0
              description: 'When using Predicted Outputs, the number of tokens in
                the

                prediction that appeared in the completion.

                '
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens generated by the model.
            reasoning_tokens:
              type: integer
              default: 0
              description: Tokens generated by the model for reasoning.
            rejected_prediction_tokens:
              type: integer
              default: 0
              description: 'When using Predicted Outputs, the number of tokens in
                the

                prediction that did not appear in the completion. However, like

                reasoning tokens, these tokens are still counted in the total

                completion tokens for purposes of billing, output, and context window

                limits.

                '
        prompt_tokens_details:
          type: object
          description: Breakdown of tokens used in the prompt.
          properties:
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens present in the prompt.
            cached_tokens:
              type: integer
              default: 0
              description: Cached tokens present in the prompt.
      required:
      - prompt_tokens
      - completion_tokens
      - total_tokens
    CreateChatCompletionRequest:
      allOf:
      - $ref: '#/components/schemas/CreateModelResponseProperties'
      - type: object
        properties:
          messages:
            description: 'A list of messages comprising the conversation so far. Depending
              on the

              [model](/docs/models) you use, different message types (modalities)
              are

              supported, like [text](/docs/guides/text-generation),

              [images](/docs/guides/vision), and [audio](/docs/guides/audio).

              '
            type: array
            minItems: 1
            items:
              $ref: '#/components/schemas/ChatCompletionRequestMessage'
          modalities:
            $ref: '#/components/schemas/ResponseModalities'
          reasoning_effort:
            $ref: '#/components/schemas/ReasoningEffort'
          max_completion_tokens:
            description: 'An upper bound for the number of tokens that can be generated
              for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).

              '
            type: integer
            nullable: true
          frequency_penalty:
            type: number
            default: 0
            minimum: -2
            maximum: 2
            nullable: true
            description: 'Number between -2.0 and 2.0. Positive values penalize new
              tokens based on

              their existing frequency in the text so far, decreasing the model''s

              likelihood to repeat the same line verbatim.

              '
          presence_penalty:
            type: number
            default: 0
            minimum: -2
            maximum: 2
            nullable: true
            description: 'Number between -2.0 and 2.0. Positive values penalize new
              tokens based on

              whether they appear in the text so far, increasing the model''s likelihood

              to talk about new topics.

              '
          web_search_options:
            type: object
            title: Web search
            description: 'This tool searches the web for relevant results to use in
              a response.

              Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).

              '
            properties:
              user_location:
                type: object
                nullable: true
                required:
                - type
                - approximate
                description: 'Approximate location parameters for the search.

                  '
                properties:
                  type:
                    type: string
                    description: 'The type of location approximation. Always `approximate`.

                      '
                    enum:
                    - approximate
                    x-stainless-const: true
                  approximate:
                    $ref: '#/components/schemas/WebSearchLocation'
              search_context_size:
                $ref: '#/components/schemas/WebSearchContextSize'
          top_logprobs:
            description: 'An integer between 0 and 20 specifying the number of most
              likely tokens to

              return at each token position, each with an associated log probability.

              `logprobs` must be set to `true` if this parameter is used.

              '
            type: integer
            minimum: 0
            maximum: 20
            nullable: true
          response_format:
            description: 'An object specifying the format that the model must output.


              Setting to `{ "type": "json_schema", "json_schema": {...} }` enables

              Structured Outputs which ensures the model will match your supplied
              JSON

              schema. Learn more in the [Structured Outputs

              guide](/docs/guides/structured-outputs).


              Setting to `{ "type": "json_object" }` enables the older JSON mode,
              which

              ensures the message the model generates is valid JSON. Using `json_schema`

              is preferred for models that support it.

              '
            oneOf:
            - $ref: '#/components/schemas/ResponseFormatText'
            - $ref: '#/components/schemas/ResponseFormatJsonSchema'
            - $ref: '#/components/schemas/ResponseFormatJsonObject'
            x-oaiExpandable: true
          service_tier:
            description: "Specifies the latency tier to use for processing the request.\
              \ This parameter is relevant for customers subscribed to the scale tier\
              \ service:\n  - If set to 'auto', and the Project is Scale tier enabled,\
              \ the system\n    will utilize scale tier credits until they are exhausted.\n\
              \  - If set to 'auto', and the Project is not Scale tier enabled, the\
              \ request will be processed using the default service tier with a lower\
              \ uptime SLA and no latency guarentee.\n  - If set to 'default', the\
              \ request will be processed using the default service tier with a lower\
              \ uptime SLA and no latency guarentee.\n  - When not set, the default\
              \ behavior is 'auto'.\n\n  When this parameter is set, the response\
              \ body will include the `service_tier` utilized.\n"
            type: string
            enum:
            - auto
            - default
            nullable: true
            default: auto
          audio:
            type: object
            nullable: true
            description: 'Parameters for audio output. Required when audio output
              is requested with

              `modalities: ["audio"]`. [Learn more](/docs/guides/audio).

              '
            required:
            - voice
            - format
            x-oaiExpandable: true
            properties:
              voice:
                type: string
                enum:
                - alloy
                - ash
                - ballad
                - coral
                - echo
                - sage
                - shimmer
                - verse
                description: "The voice the model uses to respond. Supported voices\
                  \ are \n`alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, and `shimmer`.\n"
              format:
                type: string
                enum:
                - wav
                - mp3
                - flac
                - opus
                - pcm16
                description: 'Specifies the output audio format. Must be one of `wav`,
                  `mp3`, `flac`,

                  `opus`, or `pcm16`.

                  '
          store:
            type: boolean
            default: false
            nullable: true
            description: "Whether or not to store the output of this chat completion\
              \ request for \nuse in our [model distillation](/docs/guides/distillation)\
              \ or\n[evals](/docs/guides/evals) products.\n"
          stream:
            description: 'If set to true, the model response data will be streamed
              to the client

              as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).

              See the [Streaming section below](/docs/api-reference/chat/streaming)

              for more information, along with the [streaming responses](/docs/guides/streaming-responses)

              guide for more information on how to handle the streaming events.

              '
            type: boolean
            nullable: true
            default: false
          stop:
            $ref: '#/components/schemas/StopConfiguration'
          logit_bias:
            type: object
            x-oaiTypeLabel: map
            default: null
            nullable: true
            additionalProperties:
              type: integer
            description: 'Modify the likelihood of specified tokens appearing in the
              completion.


              Accepts a JSON object that maps tokens (specified by their token ID
              in the

              tokenizer) to an associated bias value from -100 to 100. Mathematically,

              the bias is added to the logits generated by the model prior to sampling.

              The exact effect will vary per model, but values between -1 and 1 should

              decrease or increase likelihood of selection; values like -100 or 100

              should result in a ban or exclusive selection of the relevant token.

              '
          logprobs:
            description: 'Whether to return log probabilities of the output tokens
              or not. If true,

              returns the log probabilities of each output token returned in the

              `content` of `message`.

              '
            type: boolean
            default: false
            nullable: true
          max_tokens:
            description: 'The maximum number of [tokens](/tokenizer) that can be generated
              in the

              chat completion. This value can be used to control

              [costs](https://openai.com/api/pricing/) for text generated via API.


              This value is now deprecated in favor of `max_completion_tokens`, and
              is

              not compatible with [o1 series models](/docs/guides/reasoning).

              '
            type: integer
            nullable: true
            deprecated: true
          n:
            type: integer
            minimum: 1
            maximum: 128
            default: 1
            example: 1
            nullable: true
            description: How many chat completion choices to generate for each input
              message. Note that you will be charged based on the number of generated
              tokens across all of the choices. Keep `n` as `1` to minimize costs.
          prediction:
            nullable: true
            x-oaiExpandable: true
            description: 'Configuration for a [Predicted Output](/docs/guides/predicted-outputs),

              which can greatly improve response times when large parts of the model

              response are known ahead of time. This is most common when you are

              regenerating a file with only minor changes to most of the content.

              '
            oneOf:
            - $ref: '#/components/schemas/PredictionContent'
          seed:
            type: integer
            minimum: -92233720368547760
            maximum: 92233720368547760
            nullable: true
            description: 'This feature is in Beta.

              If specified, our system will make a best effort to sample deterministically,
              such that repeated requests with the same `seed` and parameters should
              return the same result.

              Determinism is not guaranteed, and you should refer to the `system_fingerprint`
              response parameter to monitor changes in the backend.

              '
            x-oaiMeta:
              beta: true
          stream_options:
            $ref: '#/components/schemas/ChatCompletionStreamOptions'
          tools:
            type: array
            description: 'A list of tools the model may call. Currently, only functions
              are supported as a tool. Use this to provide a list of functions the
              model may generate JSON inputs for. A max of 128 functions are supported.

              '
            items:
              $ref: '#/components/schemas/ChatCompletionTool'
          tool_choice:
            $ref: '#/components/schemas/ChatCompletionToolChoiceOption'
          parallel_tool_calls:
            $ref: '#/components/schemas/ParallelToolCalls'
          function_call:
            deprecated: true
            description: 'Deprecated in favor of `tool_choice`.


              Controls which (if any) function is called by the model.


              `none` means the model will not call a function and instead generates
              a

              message.


              `auto` means the model can pick between generating a message or calling
              a

              function.


              Specifying a particular function via `{"name": "my_function"}` forces
              the

              model to call that function.


              `none` is the default when no functions are present. `auto` is the default

              if functions are present.

              '
            oneOf:
            - type: string
              description: '`none` means the model will not call a function and instead
                generates a message. `auto` means the model can pick between generating
                a message or calling a function.

                '
              enum:
              - none
              - auto
            - $ref: '#/components/schemas/ChatCompletionFunctionCallOption'
            x-oaiExpandable: true
          functions:
            deprecated: true
            description: 'Deprecated in favor of `tools`.


              A list of functions the model may generate JSON inputs for.

              '
            type: array
            minItems: 1
            maxItems: 128
            items:
              $ref: '#/components/schemas/ChatCompletionFunctions'
        required:
        - model
        - messages
    CreateChatCompletionResponse:
      type: object
      description: Represents a chat completion response returned by model, based
        on the provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        choices:
          type: array
          description: A list of chat completion choices. Can be more than one if
            `n` is greater than 1.
          items:
            type: object
            required:
            - finish_reason
            - index
            - message
            - logprobs
            properties:
              finish_reason:
                type: string
                description: 'The reason the model stopped generating tokens. This
                  will be `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the request
                  was reached,

                  `content_filter` if content was omitted due to a flag from our content
                  filters,

                  `tool_calls` if the model called a tool, or `function_call` (deprecated)
                  if the model called a function.

                  '
                enum:
                - stop
                - length
                - tool_calls
                - content_filter
                - function_call
              index:
                type: integer
                description: The index of the choice in the list of choices.
              message:
                $ref: '#/components/schemas/ChatCompletionResponseMessage'
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: A list of message content tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                  refusal:
                    description: A list of message refusal tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                required:
                - content
                - refusal
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion
            was created.
        model:
          type: string
          description: The model used for the chat completion.
        service_tier:
          description: The service tier used for processing the request.
          type: string
          enum:
          - scale
          - default
          example: scale
          nullable: true
        system_fingerprint:
          type: string
          description: 'This fingerprint represents the backend configuration that
            the model runs with.


            Can be used in conjunction with the `seed` request parameter to understand
            when backend changes have been made that might impact determinism.

            '
        object:
          type: string
          description: The object type, which is always `chat.completion`.
          enum:
          - chat.completion
          x-stainless-const: true
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      required:
      - choices
      - created
      - id
      - model
      - object
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: "{\n  \"id\": \"chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG\",\n  \"object\"\
          : \"chat.completion\",\n  \"created\": 1741570283,\n  \"model\": \"gpt-4o-2024-08-06\"\
          ,\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n\
          \        \"role\": \"assistant\",\n        \"content\": \"The image shows\
          \ a wooden boardwalk path running through a lush green field or meadow.\
          \ The sky is bright blue with some scattered clouds, giving the scene a\
          \ serene and peaceful atmosphere. Trees and shrubs are visible in the background.\"\
          ,\n        \"refusal\": null,\n        \"annotations\": []\n      },\n \
          \     \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n\
          \  \"usage\": {\n    \"prompt_tokens\": 1117,\n    \"completion_tokens\"\
          : 46,\n    \"total_tokens\": 1163,\n    \"prompt_tokens_details\": {\n \
          \     \"cached_tokens\": 0,\n      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\"\
          : {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"\
          accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\":\
          \ 0\n    }\n  },\n  \"service_tier\": \"default\",\n  \"system_fingerprint\"\
          : \"fp_fc9f1d7035\"\n}\n"
    CreateChatCompletionStreamResponse:
      type: object
      description: "Represents a streamed chunk of a chat completion response returned\n\
        by the model, based on the provided input. \n[Learn more](/docs/guides/streaming-responses).\n"
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion. Each chunk has
            the same ID.
        choices:
          type: array
          description: 'A list of chat completion choices. Can contain more than one
            elements if `n` is greater than 1. Can also be empty for the

            last chunk if you set `stream_options: {"include_usage": true}`.

            '
          items:
            type: object
            required:
            - delta
            - finish_reason
            - index
            properties:
              delta:
                $ref: '#/components/schemas/ChatCompletionStreamResponseDelta'
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: A list of message content tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                  refusal:
                    description: A list of message refusal tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                required:
                - content
                - refusal
              finish_reason:
                type: string
                description: 'The reason the model stopped generating tokens. This
                  will be `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the request
                  was reached,

                  `content_filter` if content was omitted due to a flag from our content
                  filters,

                  `tool_calls` if the model called a tool, or `function_call` (deprecated)
                  if the model called a function.

                  '
                enum:
                - stop
                - length
                - tool_calls
                - content_filter
                - function_call
                nullable: true
              index:
                type: integer
                description: The index of the choice in the list of choices.
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion
            was created. Each chunk has the same timestamp.
        model:
          type: string
          description: The model to generate the completion.
        service_tier:
          description: The service tier used for processing the request.
          type: string
          enum:
          - scale
          - default
          example: scale
          nullable: true
        system_fingerprint:
          type: string
          description: 'This fingerprint represents the backend configuration that
            the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand
            when backend changes have been made that might impact determinism.

            '
        object:
          type: string
          description: The object type, which is always `chat.completion.chunk`.
          enum:
          - chat.completion.chunk
          x-stainless-const: true
        usage:
          type: object
          nullable: true
          description: 'An optional field that will only be present when you set `stream_options:
            {"include_usage": true}` in your request.

            When present, it contains a null value except for the last chunk which
            contains the token usage statistics for the entire request.

            '
          properties:
            completion_tokens:
              type: integer
              description: Number of tokens in the generated completion.
            prompt_tokens:
              type: integer
              description: Number of tokens in the prompt.
            total_tokens:
              type: integer
              description: Total number of tokens used in the request (prompt + completion).
          required:
          - prompt_tokens
          - completion_tokens
          - total_tokens
      required:
      - choices
      - created
      - id
      - model
      - object
      x-oaiMeta:
        name: The chat completion chunk object
        group: chat
        example: '{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


          ....


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}

          '
    CreateEmbeddingRequest:
      type: object
      additionalProperties: false
      properties:
        input:
          description: 'Input text to embed, encoded as a string or array of tokens.
            To embed multiple inputs in a single request, pass an array of strings
            or array of token arrays. The input must not exceed the max input tokens
            for the model (8192 tokens for `text-embedding-ada-002`), cannot be an
            empty string, and any array must be 2048 dimensions or less. [Example
            Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
            for counting tokens. Some models may also impose a limit on total number
            of tokens summed across inputs.

            '
          example: The quick brown fox jumped over the lazy dog
          oneOf:
          - type: string
            title: string
            description: The string that will be turned into an embedding.
            default: ''
            example: This is a test.
          - type: array
            title: array
            description: The array of strings that will be turned into an embedding.
            minItems: 1
            maxItems: 2048
            items:
              type: string
              default: ''
              example: '[''This is a test.'']'
          - type: array
            title: array
            description: The array of integers that will be turned into an embedding.
            minItems: 1
            maxItems: 2048
            items:
              type: integer
            example: '[1212, 318, 257, 1332, 13]'
          - type: array
            title: array
            description: The array of arrays containing integers that will be turned
              into an embedding.
            minItems: 1
            maxItems: 2048
            items:
              type: array
              minItems: 1
              items:
                type: integer
            example: '[[1212, 318, 257, 1332, 13]]'
          x-oaiExpandable: true
        model:
          description: 'ID of the model to use. You can use the [List models](/docs/api-reference/models/list)
            API to see all of your available models, or see our [Model overview](/docs/models)
            for descriptions of them.

            '
          example: text-embedding-3-small
          anyOf:
          - type: string
          - type: string
            enum:
            - text-embedding-ada-002
            - text-embedding-3-small
            - text-embedding-3-large
          x-oaiTypeLabel: string
        encoding_format:
          description: The format to return the embeddings in. Can be either `float`
            or [`base64`](https://pypi.org/project/pybase64/).
          example: float
          default: float
          type: string
          enum:
          - float
          - base64
        dimensions:
          description: 'The number of dimensions the resulting output embeddings should
            have. Only supported in `text-embedding-3` and later models.

            '
          type: integer
          minimum: 1
        user:
          type: string
          example: user-1234
          description: 'A unique identifier representing your end-user, which can
            help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).

            '
      required:
      - model
      - input
    CreateEmbeddingResponse:
      type: object
      properties:
        data:
          type: array
          description: The list of embeddings generated by the model.
          items:
            $ref: '#/components/schemas/Embedding'
        model:
          type: string
          description: The name of the model used to generate the embedding.
        object:
          type: string
          description: The object type, which is always "list".
          enum:
          - list
          x-stainless-const: true
        usage:
          type: object
          description: The usage information for the request.
          properties:
            prompt_tokens:
              type: integer
              description: The number of tokens used by the prompt.
            total_tokens:
              type: integer
              description: The total number of tokens used by the request.
          required:
          - prompt_tokens
          - total_tokens
      required:
      - object
      - model
      - data
      - usage
    CreateFileRequest:
      type: object
      additionalProperties: false
      properties:
        file:
          description: 'The File object (not file name) to be uploaded.

            '
          type: string
          format: binary
        purpose:
          description: 'The intended purpose of the uploaded file. One of: - `assistants`:
            Used in the Assistants API - `batch`: Used in the Batch API - `fine-tune`:
            Used for fine-tuning - `vision`: Images used for vision fine-tuning -
            `user_data`: Flexible file type for any purpose - `evals`: Used for eval
            data sets

            '
          type: string
          enum:
          - assistants
          - batch
          - fine-tune
          - vision
          - user_data
          - evals
      required:
      - file
      - purpose
    CreateMessageRequest:
      type: object
      additionalProperties: false
      required:
      - role
      - content
      properties:
        role:
          type: string
          enum:
          - user
          - assistant
          description: 'The role of the entity that is creating the message. Allowed
            values include:

            - `user`: Indicates the message is sent by an actual user and should be
            used in most cases to represent user-generated messages.

            - `assistant`: Indicates the message is generated by the assistant. Use
            this value to insert messages from the assistant into the conversation.

            '
        content:
          oneOf:
          - type: string
            description: The text contents of the message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type, each can be
              of type `text` or images can be passed with `image_url` or `image_file`.
              Image types are only supported on [Vision-compatible models](/docs/models).
            title: Array of content parts
            items:
              oneOf:
              - $ref: '#/components/schemas/MessageContentImageFileObject'
              - $ref: '#/components/schemas/MessageContentImageUrlObject'
              - $ref: '#/components/schemas/MessageRequestContentTextObject'
              x-oaiExpandable: true
            minItems: 1
          x-oaiExpandable: true
        attachments:
          type: array
          items:
            type: object
            properties:
              file_id:
                type: string
                description: The ID of the file to attach to the message.
              tools:
                description: The tools to add this file to.
                type: array
                items:
                  oneOf:
                  - $ref: '#/components/schemas/AssistantToolsCode'
                  - $ref: '#/components/schemas/AssistantToolsFileSearchTypeOnly'
                  x-oaiExpandable: true
          description: A list of files attached to the message, and the tools they
            should be added to.
          required:
          - file_id
          - tools
          nullable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
    CreateModelResponseProperties:
      allOf:
      - $ref: '#/components/schemas/ModelResponseProperties'
    CreateRunRequest:
      type: object
      additionalProperties: false
      properties:
        assistant_id:
          description: The ID of the [assistant](/docs/api-reference/assistants) to
            use to execute this run.
          type: string
        model:
          description: The ID of the [Model](/docs/api-reference/models) to be used
            to execute this run. If a value is provided here, it will override the
            model associated with the assistant. If not, the model associated with
            the assistant will be used.
          example: gpt-4o
          anyOf:
          - type: string
          - $ref: '#/components/schemas/AssistantSupportedModels'
          x-oaiTypeLabel: string
          nullable: true
        reasoning_effort:
          $ref: '#/components/schemas/ReasoningEffort'
        instructions:
          description: Overrides the [instructions](/docs/api-reference/assistants/createAssistant)
            of the assistant. This is useful for modifying the behavior on a per-run
            basis.
          type: string
          nullable: true
        additional_instructions:
          description: Appends additional instructions at the end of the instructions
            for the run. This is useful for modifying the behavior on a per-run basis
            without overriding other instructions.
          type: string
          nullable: true
        additional_messages:
          description: Adds additional messages to the thread before creating the
            run.
          type: array
          items:
            $ref: '#/components/schemas/CreateMessageRequest'
          nullable: true
        tools:
          description: Override the tools the assistant can use for this run. This
            is useful for modifying the behavior on a per-run basis.
          nullable: true
          type: array
          maxItems: 20
          items:
            oneOf:
            - $ref: '#/components/schemas/AssistantToolsCode'
            - $ref: '#/components/schemas/AssistantToolsFileSearch'
            - $ref: '#/components/schemas/AssistantToolsFunction'
            x-oaiExpandable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: 'What sampling temperature to use, between 0 and 2. Higher
            values like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.

            '
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: 'An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with top_p
            probability mass. So 0.1 means only the tokens comprising the top 10%
            probability mass are considered.


            We generally recommend altering this or temperature but not both.

            '
        stream:
          type: boolean
          nullable: true
          description: 'If `true`, returns a stream of events that happen during the
            Run as server-sent events, terminating when the Run enters a terminal
            state with a `data: [DONE]` message.

            '
        max_prompt_tokens:
          type: integer
          nullable: true
          description: 'The maximum number of prompt tokens that may be used over
            the course of the run. The run will make a best effort to use only the
            number of prompt tokens specified, across multiple turns of the run. If
            the run exceeds the number of prompt tokens specified, the run will end
            with status `incomplete`. See `incomplete_details` for more info.

            '
          minimum: 256
        max_completion_tokens:
          type: integer
          nullable: true
          description: 'The maximum number of completion tokens that may be used over
            the course of the run. The run will make a best effort to use only the
            number of completion tokens specified, across multiple turns of the run.
            If the run exceeds the number of completion tokens specified, the run
            will end with status `incomplete`. See `incomplete_details` for more info.

            '
          minimum: 256
        truncation_strategy:
          allOf:
          - $ref: '#/components/schemas/TruncationObject'
          - nullable: true
        tool_choice:
          allOf:
          - $ref: '#/components/schemas/AssistantsApiToolChoiceOption'
          - nullable: true
        parallel_tool_calls:
          $ref: '#/components/schemas/ParallelToolCalls'
        response_format:
          $ref: '#/components/schemas/AssistantsApiResponseFormatOption'
          nullable: true
      required:
      - assistant_id
    CreateThreadAndRunRequest:
      type: object
      additionalProperties: false
      properties:
        assistant_id:
          description: The ID of the [assistant](/docs/api-reference/assistants) to
            use to execute this run.
          type: string
        thread:
          $ref: '#/components/schemas/CreateThreadRequest'
        model:
          description: The ID of the [Model](/docs/api-reference/models) to be used
            to execute this run. If a value is provided here, it will override the
            model associated with the assistant. If not, the model associated with
            the assistant will be used.
          example: gpt-4o
          anyOf:
          - type: string
          - type: string
            enum:
            - gpt-4o
            - gpt-4o-2024-11-20
            - gpt-4o-2024-08-06
            - gpt-4o-2024-05-13
            - gpt-4o-mini
            - gpt-4o-mini-2024-07-18
            - gpt-4.5-preview
            - gpt-4.5-preview-2025-02-27
            - gpt-4-turbo
            - gpt-4-turbo-2024-04-09
            - gpt-4-0125-preview
            - gpt-4-turbo-preview
            - gpt-4-1106-preview
            - gpt-4-vision-preview
            - gpt-4
            - gpt-4-0314
            - gpt-4-0613
            - gpt-4-32k
            - gpt-4-32k-0314
            - gpt-4-32k-0613
            - gpt-3.5-turbo
            - gpt-3.5-turbo-16k
            - gpt-3.5-turbo-0613
            - gpt-3.5-turbo-1106
            - gpt-3.5-turbo-0125
            - gpt-3.5-turbo-16k-0613
          x-oaiTypeLabel: string
          nullable: true
        instructions:
          description: Override the default system message of the assistant. This
            is useful for modifying the behavior on a per-run basis.
          type: string
          nullable: true
        tools:
          description: Override the tools the assistant can use for this run. This
            is useful for modifying the behavior on a per-run basis.
          nullable: true
          type: array
          maxItems: 20
          items:
            oneOf:
            - $ref: '#/components/schemas/AssistantToolsCode'
            - $ref: '#/components/schemas/AssistantToolsFileSearch'
            - $ref: '#/components/schemas/AssistantToolsFunction'
        tool_resources:
          type: object
          description: 'A set of resources that are used by the assistant''s tools.
            The resources are specific to the type of tool. For example, the `code_interpreter`
            tool requires a list of file IDs, while the `file_search` tool requires
            a list of vector store IDs.

            '
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: 'A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a maximum
                    of 20 files associated with the tool.

                    '
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: 'The ID of the [vector store](/docs/api-reference/vector-stores/object)
                    attached to this assistant. There can be a maximum of 1 vector
                    store attached to the assistant.

                    '
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: 'What sampling temperature to use, between 0 and 2. Higher
            values like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.

            '
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: 'An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with top_p
            probability mass. So 0.1 means only the tokens comprising the top 10%
            probability mass are considered.


            We generally recommend altering this or temperature but not both.

            '
        stream:
          type: boolean
          nullable: true
          description: 'If `true`, returns a stream of events that happen during the
            Run as server-sent events, terminating when the Run enters a terminal
            state with a `data: [DONE]` message.

            '
        max_prompt_tokens:
          type: integer
          nullable: true
          description: 'The maximum number of prompt tokens that may be used over
            the course of the run. The run will make a best effort to use only the
            number of prompt tokens specified, across multiple turns of the run. If
            the run exceeds the number of prompt tokens specified, the run will end
            with status `incomplete`. See `incomplete_details` for more info.

            '
          minimum: 256
        max_completion_tokens:
          type: integer
          nullable: true
          description: 'The maximum number of completion tokens that may be used over
            the course of the run. The run will make a best effort to use only the
            number of completion tokens specified, across multiple turns of the run.
            If the run exceeds the number of completion tokens specified, the run
            will end with status `incomplete`. See `incomplete_details` for more info.

            '
          minimum: 256
        truncation_strategy:
          allOf:
          - $ref: '#/components/schemas/TruncationObject'
          - nullable: true
        tool_choice:
          allOf:
          - $ref: '#/components/schemas/AssistantsApiToolChoiceOption'
          - nullable: true
        parallel_tool_calls:
          $ref: '#/components/schemas/ParallelToolCalls'
        response_format:
          $ref: '#/components/schemas/AssistantsApiResponseFormatOption'
          nullable: true
      required:
      - assistant_id
    CreateThreadRequest:
      type: object
      description: "Options to create a new thread. If no thread is provided when\
        \ running a \nrequest, an empty thread will be created.\n"
      additionalProperties: false
      properties:
        messages:
          description: A list of [messages](/docs/api-reference/messages) to start
            the thread with.
          type: array
          items:
            $ref: '#/components/schemas/CreateMessageRequest'
        tool_resources:
          type: object
          description: 'A set of resources that are made available to the assistant''s
            tools in this thread. The resources are specific to the type of tool.
            For example, the `code_interpreter` tool requires a list of file IDs,
            while the `file_search` tool requires a list of vector store IDs.

            '
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: 'A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a maximum
                    of 20 files associated with the tool.

                    '
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: 'The [vector store](/docs/api-reference/vector-stores/object)
                    attached to this thread. There can be a maximum of 1 vector store
                    attached to the thread.

                    '
                  maxItems: 1
                  items:
                    type: string
                vector_stores:
                  type: array
                  description: 'A helper to create a [vector store](/docs/api-reference/vector-stores/object)
                    with file_ids and attach it to this thread. There can be a maximum
                    of 1 vector store attached to the thread.

                    '
                  maxItems: 1
                  items:
                    type: object
                    properties:
                      file_ids:
                        type: array
                        description: 'A list of [file](/docs/api-reference/files)
                          IDs to add to the vector store. There can be a maximum of
                          10000 files in a vector store.

                          '
                        maxItems: 10000
                        items:
                          type: string
                      chunking_strategy:
                        type: object
                        description: The chunking strategy used to chunk the file(s).
                          If not set, will use the `auto` strategy.
                        oneOf:
                        - type: object
                          title: Auto Chunking Strategy
                          description: The default strategy. This strategy currently
                            uses a `max_chunk_size_tokens` of `800` and `chunk_overlap_tokens`
                            of `400`.
                          additionalProperties: false
                          properties:
                            type:
                              type: string
                              description: Always `auto`.
                              enum:
                              - auto
                              x-stainless-const: true
                          required:
                          - type
                        - type: object
                          title: Static Chunking Strategy
                          additionalProperties: false
                          properties:
                            type:
                              type: string
                              description: Always `static`.
                              enum:
                              - static
                              x-stainless-const: true
                            static:
                              type: object
                              additionalProperties: false
                              properties:
                                max_chunk_size_tokens:
                                  type: integer
                                  minimum: 100
                                  maximum: 4096
                                  description: The maximum number of tokens in each
                                    chunk. The default value is `800`. The minimum
                                    value is `100` and the maximum value is `4096`.
                                chunk_overlap_tokens:
                                  type: integer
                                  description: 'The number of tokens that overlap
                                    between chunks. The default value is `400`.


                                    Note that the overlap must not exceed half of
                                    `max_chunk_size_tokens`.

                                    '
                              required:
                              - max_chunk_size_tokens
                              - chunk_overlap_tokens
                          required:
                          - type
                          - static
                        x-oaiExpandable: true
                      metadata:
                        $ref: '#/components/schemas/Metadata'
                    x-oaiExpandable: true
              oneOf:
              - required:
                - vector_store_ids
              - required:
                - vector_stores
          nullable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
    CreateVectorStoreFileRequest:
      type: object
      additionalProperties: false
      properties:
        file_id:
          description: A [File](/docs/api-reference/files) ID that the vector store
            should use. Useful for tools like `file_search` that can access files.
          type: string
        chunking_strategy:
          $ref: '#/components/schemas/ChunkingStrategyRequestParam'
        attributes:
          $ref: '#/components/schemas/VectorStoreFileAttributes'
      required:
      - file_id
    CreateVectorStoreRequest:
      type: object
      additionalProperties: false
      properties:
        file_ids:
          description: A list of [File](/docs/api-reference/files) IDs that the vector
            store should use. Useful for tools like `file_search` that can access
            files.
          type: array
          maxItems: 500
          items:
            type: string
        name:
          description: The name of the vector store.
          type: string
        expires_after:
          $ref: '#/components/schemas/VectorStoreExpirationAfter'
        chunking_strategy:
          type: object
          description: The chunking strategy used to chunk the file(s). If not set,
            will use the `auto` strategy. Only applicable if `file_ids` is non-empty.
          oneOf:
          - $ref: '#/components/schemas/AutoChunkingStrategyRequestParam'
          - $ref: '#/components/schemas/StaticChunkingStrategyRequestParam'
          x-oaiExpandable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
    DeleteFileResponse:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
          enum:
          - file
          x-stainless-const: true
        deleted:
          type: boolean
      required:
      - id
      - object
      - deleted
    DeleteVectorStoreFileResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
          enum:
          - vector_store.file.deleted
          x-stainless-const: true
      required:
      - id
      - object
      - deleted
    Embedding:
      type: object
      description: 'Represents an embedding vector returned by embedding endpoint.

        '
      properties:
        index:
          type: integer
          description: The index of the embedding in the list of embeddings.
        embedding:
          type: array
          description: 'The embedding vector, which is a list of floats. The length
            of vector depends on the model as listed in the [embedding guide](/docs/guides/embeddings).

            '
          items:
            type: number
        object:
          type: string
          description: The object type, which is always "embedding".
          enum:
          - embedding
          x-stainless-const: true
      required:
      - index
      - object
      - embedding
      x-oaiMeta:
        name: The embedding object
        example: "{\n  \"object\": \"embedding\",\n  \"embedding\": [\n    0.0023064255,\n\
          \    -0.009327292,\n    .... (1536 floats total for ada-002)\n    -0.0028842222,\n\
          \  ],\n  \"index\": 0\n}\n"
    FileSearchRanker:
      type: string
      description: The ranker to use for the file search. If not specified will use
        the `auto` ranker.
      enum:
      - auto
      - default_2024_08_21
    FileSearchRankingOptions:
      title: File search tool call ranking options
      type: object
      description: 'The ranking options for the file search. If not specified, the
        file search tool will use the `auto` ranker and a score_threshold of 0.


        See the [file search tool documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
        for more information.

        '
      properties:
        ranker:
          $ref: '#/components/schemas/FileSearchRanker'
        score_threshold:
          type: number
          description: The score threshold for the file search. All values must be
            a floating point number between 0 and 1.
          minimum: 0
          maximum: 1
      required:
      - score_threshold
    FunctionObject:
      type: object
      properties:
        description:
          type: string
          description: A description of what the function does, used by the model
            to choose when and how to call the function.
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9,
            or contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: '#/components/schemas/FunctionParameters'
        strict:
          type: boolean
          nullable: true
          default: false
          description: Whether to enable strict schema adherence when generating the
            function call. If set to true, the model will follow the exact schema
            defined in the `parameters` field. Only a subset of JSON Schema is supported
            when `strict` is `true`. Learn more about Structured Outputs in the [function
            calling guide](docs/guides/function-calling).
      required:
      - name
    FunctionParameters:
      type: object
      description: "The parameters the functions accepts, described as a JSON Schema\
        \ object. See the [guide](/docs/guides/function-calling) for examples, and\
        \ the [JSON Schema reference](https://json-schema.org/understanding-json-schema/)\
        \ for documentation about the format. \n\nOmitting `parameters` defines a\
        \ function with an empty parameter list."
      additionalProperties: true
    ListAssistantsResponse:
      type: object
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: '#/components/schemas/AssistantObject'
        first_id:
          type: string
          example: asst_abc123
        last_id:
          type: string
          example: asst_abc456
        has_more:
          type: boolean
          example: false
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
      x-oaiMeta:
        name: List assistants response object
        group: chat
        example: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\":\
          \ \"asst_abc123\",\n      \"object\": \"assistant\",\n      \"created_at\"\
          : 1698982736,\n      \"name\": \"Coding Tutor\",\n      \"description\"\
          : null,\n      \"model\": \"gpt-4o\",\n      \"instructions\": \"You are\
          \ a helpful assistant designed to make me better at coding!\",\n      \"\
          tools\": [],\n      \"tool_resources\": {},\n      \"metadata\": {},\n \
          \     \"top_p\": 1.0,\n      \"temperature\": 1.0,\n      \"response_format\"\
          : \"auto\"\n    },\n    {\n      \"id\": \"asst_abc456\",\n      \"object\"\
          : \"assistant\",\n      \"created_at\": 1698982718,\n      \"name\": \"\
          My Assistant\",\n      \"description\": null,\n      \"model\": \"gpt-4o\"\
          ,\n      \"instructions\": \"You are a helpful assistant designed to make\
          \ me better at coding!\",\n      \"tools\": [],\n      \"tool_resources\"\
          : {},\n      \"metadata\": {},\n      \"top_p\": 1.0,\n      \"temperature\"\
          : 1.0,\n      \"response_format\": \"auto\"\n    },\n    {\n      \"id\"\
          : \"asst_abc789\",\n      \"object\": \"assistant\",\n      \"created_at\"\
          : 1698982643,\n      \"name\": null,\n      \"description\": null,\n   \
          \   \"model\": \"gpt-4o\",\n      \"instructions\": null,\n      \"tools\"\
          : [],\n      \"tool_resources\": {},\n      \"metadata\": {},\n      \"\
          top_p\": 1.0,\n      \"temperature\": 1.0,\n      \"response_format\": \"\
          auto\"\n    }\n  ],\n  \"first_id\": \"asst_abc123\",\n  \"last_id\": \"\
          asst_abc789\",\n  \"has_more\": false\n}\n"
    ListFilesResponse:
      type: object
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIFile'
        first_id:
          type: string
          example: file-abc123
        last_id:
          type: string
          example: file-abc456
        has_more:
          type: boolean
          example: false
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
    ListMessagesResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: '#/components/schemas/MessageObject'
        first_id:
          type: string
          example: msg_abc123
        last_id:
          type: string
          example: msg_abc123
        has_more:
          type: boolean
          example: false
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
    ListModelsResponse:
      type: object
      properties:
        object:
          type: string
          enum:
          - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: '#/components/schemas/Model'
      required:
      - object
      - data
    ListRunsResponse:
      type: object
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: '#/components/schemas/RunObject'
        first_id:
          type: string
          example: run_abc123
        last_id:
          type: string
          example: run_abc456
        has_more:
          type: boolean
          example: false
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
    ListVectorStoreFilesResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: '#/components/schemas/VectorStoreFileObject'
        first_id:
          type: string
          example: file-abc123
        last_id:
          type: string
          example: file-abc456
        has_more:
          type: boolean
          example: false
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
    ListVectorStoresResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: '#/components/schemas/VectorStoreObject'
        first_id:
          type: string
          example: vs_abc123
        last_id:
          type: string
          example: vs_abc456
        has_more:
          type: boolean
          example: false
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
    MessageContentImageFileObject:
      title: Image file
      type: object
      description: References an image [File](/docs/api-reference/files) in the content
        of a message.
      properties:
        type:
          description: Always `image_file`.
          type: string
          enum:
          - image_file
          x-stainless-const: true
        image_file:
          type: object
          properties:
            file_id:
              description: The [File](/docs/api-reference/files) ID of the image in
                the message content. Set `purpose="vision"` when uploading the File
                if you need to later display the file content.
              type: string
            detail:
              type: string
              description: Specifies the detail level of the image if specified by
                the user. `low` uses fewer tokens, you can opt in to high resolution
                using `high`.
              enum:
              - auto
              - low
              - high
              default: auto
          required:
          - file_id
      required:
      - type
      - image_file
    MessageContentImageUrlObject:
      title: Image URL
      type: object
      description: References an image URL in the content of a message.
      properties:
        type:
          type: string
          enum:
          - image_url
          description: The type of the content part.
          x-stainless-const: true
        image_url:
          type: object
          properties:
            url:
              type: string
              description: 'The external URL of the image, must be a supported image
                types: jpeg, jpg, png, gif, webp.'
              format: uri
            detail:
              type: string
              description: Specifies the detail level of the image. `low` uses fewer
                tokens, you can opt in to high resolution using `high`. Default value
                is `auto`
              enum:
              - auto
              - low
              - high
              default: auto
          required:
          - url
      required:
      - type
      - image_url
    MessageContentRefusalObject:
      title: Refusal
      type: object
      description: The refusal content generated by the assistant.
      properties:
        type:
          description: Always `refusal`.
          type: string
          enum:
          - refusal
          x-stainless-const: true
        refusal:
          type: string
          nullable: false
      required:
      - type
      - refusal
    MessageContentTextAnnotationsFileCitationObject:
      title: File citation
      type: object
      description: A citation within the message that points to a specific quote from
        a specific File associated with the assistant or the message. Generated when
        the assistant uses the "file_search" tool to search files.
      properties:
        type:
          description: Always `file_citation`.
          type: string
          enum:
          - file_citation
          x-stainless-const: true
        text:
          description: The text in the message content that needs to be replaced.
          type: string
        file_citation:
          type: object
          properties:
            file_id:
              description: The ID of the specific File the citation is from.
              type: string
          required:
          - file_id
        start_index:
          type: integer
          minimum: 0
        end_index:
          type: integer
          minimum: 0
      required:
      - type
      - text
      - file_citation
      - start_index
      - end_index
    MessageContentTextAnnotationsFilePathObject:
      title: File path
      type: object
      description: A URL for the file that's generated when the assistant used the
        `code_interpreter` tool to generate a file.
      properties:
        type:
          description: Always `file_path`.
          type: string
          enum:
          - file_path
          x-stainless-const: true
        text:
          description: The text in the message content that needs to be replaced.
          type: string
        file_path:
          type: object
          properties:
            file_id:
              description: The ID of the file that was generated.
              type: string
          required:
          - file_id
        start_index:
          type: integer
          minimum: 0
        end_index:
          type: integer
          minimum: 0
      required:
      - type
      - text
      - file_path
      - start_index
      - end_index
    MessageContentTextObject:
      title: Text
      type: object
      description: The text content that is part of a message.
      properties:
        type:
          description: Always `text`.
          type: string
          enum:
          - text
          x-stainless-const: true
        text:
          type: object
          properties:
            value:
              description: The data that makes up the text.
              type: string
            annotations:
              type: array
              items:
                oneOf:
                - $ref: '#/components/schemas/MessageContentTextAnnotationsFileCitationObject'
                - $ref: '#/components/schemas/MessageContentTextAnnotationsFilePathObject'
                x-oaiExpandable: true
          required:
          - value
          - annotations
      required:
      - type
      - text
    MessageObject:
      type: object
      title: The message object
      description: Represents a message within a [thread](/docs/api-reference/threads).
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `thread.message`.
          type: string
          enum:
          - thread.message
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the message was created.
          type: integer
        thread_id:
          description: The [thread](/docs/api-reference/threads) ID that this message
            belongs to.
          type: string
        status:
          description: The status of the message, which can be either `in_progress`,
            `incomplete`, or `completed`.
          type: string
          enum:
          - in_progress
          - incomplete
          - completed
        incomplete_details:
          description: On an incomplete message, details about why the message is
            incomplete.
          type: object
          properties:
            reason:
              type: string
              description: The reason the message is incomplete.
              enum:
              - content_filter
              - max_tokens
              - run_cancelled
              - run_expired
              - run_failed
          nullable: true
          required:
          - reason
        completed_at:
          description: The Unix timestamp (in seconds) for when the message was completed.
          type: integer
          nullable: true
        incomplete_at:
          description: The Unix timestamp (in seconds) for when the message was marked
            as incomplete.
          type: integer
          nullable: true
        role:
          description: The entity that produced the message. One of `user` or `assistant`.
          type: string
          enum:
          - user
          - assistant
        content:
          description: The content of the message in array of text and/or images.
          type: array
          items:
            oneOf:
            - $ref: '#/components/schemas/MessageContentImageFileObject'
            - $ref: '#/components/schemas/MessageContentImageUrlObject'
            - $ref: '#/components/schemas/MessageContentTextObject'
            - $ref: '#/components/schemas/MessageContentRefusalObject'
            x-oaiExpandable: true
        assistant_id:
          description: If applicable, the ID of the [assistant](/docs/api-reference/assistants)
            that authored this message.
          type: string
          nullable: true
        run_id:
          description: The ID of the [run](/docs/api-reference/runs) associated with
            the creation of this message. Value is `null` when messages are created
            manually using the create message or create thread endpoints.
          type: string
          nullable: true
        attachments:
          type: array
          items:
            type: object
            properties:
              file_id:
                type: string
                description: The ID of the file to attach to the message.
              tools:
                description: The tools to add this file to.
                type: array
                items:
                  oneOf:
                  - $ref: '#/components/schemas/AssistantToolsCode'
                  - $ref: '#/components/schemas/AssistantToolsFileSearchTypeOnly'
                  x-oaiExpandable: true
          description: A list of files attached to the message, and the tools they
            were added to.
          nullable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
      required:
      - id
      - object
      - created_at
      - thread_id
      - status
      - incomplete_details
      - completed_at
      - incomplete_at
      - role
      - content
      - assistant_id
      - run_id
      - attachments
      - metadata
      x-oaiMeta:
        name: The message object
        beta: true
        example: "{\n  \"id\": \"msg_abc123\",\n  \"object\": \"thread.message\",\n\
          \  \"created_at\": 1698983503,\n  \"thread_id\": \"thread_abc123\",\n  \"\
          role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\"\
          ,\n      \"text\": {\n        \"value\": \"Hi! How can I help you today?\"\
          ,\n        \"annotations\": []\n      }\n    }\n  ],\n  \"assistant_id\"\
          : \"asst_abc123\",\n  \"run_id\": \"run_abc123\",\n  \"attachments\": [],\n\
          \  \"metadata\": {}\n}\n"
    MessageRequestContentTextObject:
      title: Text
      type: object
      description: The text content that is part of a message.
      properties:
        type:
          description: Always `text`.
          type: string
          enum:
          - text
          x-stainless-const: true
        text:
          type: string
          description: Text content to be sent to the model
      required:
      - type
      - text
    Metadata:
      type: object
      description: "Set of 16 key-value pairs that can be attached to an object. This\
        \ can be\nuseful for storing additional information about the object in a\
        \ structured\nformat, and querying for objects via API or the dashboard. \n\
        \nKeys are strings with a maximum length of 64 characters. Values are strings\n\
        with a maximum length of 512 characters.\n"
      additionalProperties:
        type: string
      x-oaiTypeLabel: map
      nullable: true
    Model:
      title: Model
      description: Describes an OpenAI model offering that can be used with the API.
      properties:
        id:
          type: string
          description: The model identifier, which can be referenced in the API endpoints.
        created:
          type: integer
          description: The Unix timestamp (in seconds) when the model was created.
        object:
          type: string
          description: The object type, which is always "model".
          enum:
          - model
          x-stainless-const: true
        owned_by:
          type: string
          description: The organization that owns the model.
      required:
      - id
      - object
      - created
      - owned_by
      x-oaiMeta:
        name: The model object
        example: "{\n  \"id\": \"VAR_chat_model_id\",\n  \"object\": \"model\",\n\
          \  \"created\": 1686935002,\n  \"owned_by\": \"openai\"\n}\n"
    ModelResponseProperties:
      type: object
      properties:
        model:
          description: 'Model ID used to generate the response, like `gpt-4o` or `o1`.
            OpenAI

            offers a wide range of models with different capabilities, performance

            characteristics, and price points. Refer to the [model guide](/docs/models)

            to browse and compare available models.

            '
          example: gpt-4o
          anyOf:
          - type: string
          - type: string
            enum:
            - o3-mini
            - o3-mini-2025-01-31
            - o1
            - o1-2024-12-17
            - o1-preview
            - o1-preview-2024-09-12
            - o1-mini
            - o1-mini-2024-09-12
            - computer-use-preview
            - computer-use-preview-2025-02-04
            - computer-use-preview-2025-03-11
            - gpt-4.5-preview
            - gpt-4.5-preview-2025-02-27
            - gpt-4o
            - gpt-4o-2024-11-20
            - gpt-4o-2024-08-06
            - gpt-4o-2024-05-13
            - gpt-4o-audio-preview
            - gpt-4o-audio-preview-2024-10-01
            - gpt-4o-audio-preview-2024-12-17
            - gpt-4o-mini-audio-preview
            - gpt-4o-mini-audio-preview-2024-12-17
            - chatgpt-4o-latest
            - gpt-4o-mini
            - gpt-4o-mini-2024-07-18
            - gpt-4-turbo
            - gpt-4-turbo-2024-04-09
            - gpt-4-0125-preview
            - gpt-4-turbo-preview
            - gpt-4-1106-preview
            - gpt-4-vision-preview
            - gpt-4
            - gpt-4-0314
            - gpt-4-0613
            - gpt-4-32k
            - gpt-4-32k-0314
            - gpt-4-32k-0613
            - gpt-3.5-turbo
            - gpt-3.5-turbo-16k
            - gpt-3.5-turbo-0301
            - gpt-3.5-turbo-0613
            - gpt-3.5-turbo-1106
            - gpt-3.5-turbo-0125
            - gpt-3.5-turbo-16k-0613
          x-oaiTypeLabel: string
        metadata:
          $ref: '#/components/schemas/Metadata'
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: 'What sampling temperature to use, between 0 and 2. Higher
            values like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.

            '
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: 'An alternative to sampling with temperature, called nucleus
            sampling,

            where the model considers the results of the tokens with top_p probability

            mass. So 0.1 means only the tokens comprising the top 10% probability
            mass

            are considered.


            We generally recommend altering this or `temperature` but not both.

            '
        user:
          type: string
          example: user-1234
          description: 'A unique identifier representing your end-user, which can
            help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).

            '
    OpenAIFile:
      title: OpenAIFile
      description: The `File` object represents a document that has been uploaded
        to OpenAI.
      properties:
        id:
          type: string
          description: The file identifier, which can be referenced in the API endpoints.
        bytes:
          type: integer
          description: The size of the file, in bytes.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the file was created.
        expires_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the file will expire.
        filename:
          type: string
          description: The name of the file.
        object:
          type: string
          description: The object type, which is always `file`.
          enum:
          - file
          x-stainless-const: true
        purpose:
          type: string
          description: The intended purpose of the file. Supported values are `assistants`,
            `assistants_output`, `batch`, `batch_output`, `fine-tune`, `fine-tune-results`
            and `vision`.
          enum:
          - assistants
          - assistants_output
          - batch
          - batch_output
          - fine-tune
          - fine-tune-results
          - vision
        status:
          type: string
          deprecated: true
          description: Deprecated. The current status of the file, which can be either
            `uploaded`, `processed`, or `error`.
          enum:
          - uploaded
          - processed
          - error
        status_details:
          type: string
          deprecated: true
          description: Deprecated. For details on why a fine-tuning training file
            failed validation, see the `error` field on `fine_tuning.job`.
      required:
      - id
      - object
      - bytes
      - created_at
      - filename
      - purpose
      - status
      x-oaiMeta:
        name: The file object
        example: "{\n  \"id\": \"file-abc123\",\n  \"object\": \"file\",\n  \"bytes\"\
          : 120000,\n  \"created_at\": 1677610602,\n  \"expires_at\": 1680202602,\n\
          \  \"filename\": \"salesOverview.pdf\",\n  \"purpose\": \"assistants\",\n\
          }\n"
    OtherChunkingStrategyResponseParam:
      type: object
      title: Other Chunking Strategy
      description: This is returned when the chunking strategy is unknown. Typically,
        this is because the file was indexed before the `chunking_strategy` concept
        was introduced in the API.
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `other`.
          enum:
          - other
          x-stainless-const: true
      required:
      - type
    ParallelToolCalls:
      description: Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling)
        during tool use.
      type: boolean
      default: true
    PredictionContent:
      type: object
      title: Static Content
      description: 'Static predicted output content, such as the content of a text
        file that is

        being regenerated.

        '
      required:
      - type
      - content
      properties:
        type:
          type: string
          enum:
          - content
          description: 'The type of the predicted content you want to provide. This
            type is

            currently always `content`.

            '
          x-stainless-const: true
        content:
          x-oaiExpandable: true
          description: 'The content that should be matched when generating a model
            response.

            If generated tokens would match this content, the entire model response

            can be returned much more quickly.

            '
          oneOf:
          - type: string
            title: Text content
            description: 'The content used for a Predicted Output. This is often the

              text of a file you are regenerating with minor changes.

              '
          - type: array
            description: An array of content parts with a defined type. Supported
              options differ based on the [model](/docs/models) being used to generate
              the response. Can contain text inputs.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
            minItems: 1
    ReasoningEffort:
      type: string
      enum:
      - low
      - medium
      - high
      default: medium
      nullable: true
      description: "**o-series models only** \n\nConstrains effort on reasoning for\
        \ \n[reasoning models](https://platform.openai.com/docs/guides/reasoning).\n\
        Currently supported values are `low`, `medium`, and `high`. Reducing\nreasoning\
        \ effort can result in faster responses and fewer tokens used\non reasoning\
        \ in a response.\n"
    ResponseFormatJsonObject:
      type: object
      title: JSON object
      description: 'JSON object response format. An older method of generating JSON
        responses.

        Using `json_schema` is recommended for models that support it. Note that the

        model will not generate JSON without a system or user message instructing
        it

        to do so.

        '
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `json_object`.
          enum:
          - json_object
          x-stainless-const: true
      required:
      - type
    ResponseFormatJsonSchema:
      type: object
      title: JSON schema
      description: 'JSON Schema response format. Used to generate structured JSON
        responses.

        Learn more about [Structured Outputs](/docs/guides/structured-outputs).

        '
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `json_schema`.
          enum:
          - json_schema
          x-stainless-const: true
        json_schema:
          type: object
          title: JSON schema
          description: 'Structured Outputs configuration options, including a JSON
            Schema.

            '
          properties:
            description:
              type: string
              description: 'A description of what the response format is for, used
                by the model to

                determine how to respond in the format.

                '
            name:
              type: string
              description: 'The name of the response format. Must be a-z, A-Z, 0-9,
                or contain

                underscores and dashes, with a maximum length of 64.

                '
            schema:
              $ref: '#/components/schemas/ResponseFormatJsonSchemaSchema'
            strict:
              type: boolean
              nullable: true
              default: false
              description: 'Whether to enable strict schema adherence when generating
                the output.

                If set to true, the model will always follow the exact schema defined

                in the `schema` field. Only a subset of JSON Schema is supported when

                `strict` is `true`. To learn more, read the [Structured Outputs

                guide](/docs/guides/structured-outputs).

                '
          required:
          - name
      required:
      - type
      - json_schema
    ResponseFormatJsonSchemaSchema:
      type: object
      title: JSON schema
      description: 'The schema for the response format, described as a JSON Schema
        object.

        Learn how to build JSON schemas [here](https://json-schema.org/).

        '
      additionalProperties: true
    ResponseFormatText:
      type: object
      title: Text
      description: 'Default response format. Used to generate text responses.

        '
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `text`.
          enum:
          - text
          x-stainless-const: true
      required:
      - type
    ResponseModalities:
      type: array
      nullable: true
      description: "Output types that you would like the model to generate.\nMost\
        \ models are capable of generating text, which is the default:\n\n`[\"text\"\
        ]`\n\nThe `gpt-4o-audio-preview` model can also be used to \n[generate audio](/docs/guides/audio).\
        \ To request that this model generate \nboth text and audio responses, you\
        \ can use:\n\n`[\"text\", \"audio\"]`\n"
      items:
        type: string
        enum:
        - text
        - audio
    RunCompletionUsage:
      type: object
      description: Usage statistics related to the run. This value will be `null`
        if the run is not in a terminal state (i.e. `in_progress`, `queued`, etc.).
      properties:
        completion_tokens:
          type: integer
          description: Number of completion tokens used over the course of the run.
        prompt_tokens:
          type: integer
          description: Number of prompt tokens used over the course of the run.
        total_tokens:
          type: integer
          description: Total number of tokens used (prompt + completion).
      required:
      - prompt_tokens
      - completion_tokens
      - total_tokens
      nullable: true
    RunObject:
      type: object
      title: A run on a thread
      description: Represents an execution run on a [thread](/docs/api-reference/threads).
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `thread.run`.
          type: string
          enum:
          - thread.run
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the run was created.
          type: integer
        thread_id:
          description: The ID of the [thread](/docs/api-reference/threads) that was
            executed on as a part of this run.
          type: string
        assistant_id:
          description: The ID of the [assistant](/docs/api-reference/assistants) used
            for execution of this run.
          type: string
        status:
          description: The status of the run, which can be either `queued`, `in_progress`,
            `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`, `incomplete`,
            or `expired`.
          type: string
          enum:
          - queued
          - in_progress
          - requires_action
          - cancelling
          - cancelled
          - failed
          - completed
          - incomplete
          - expired
        required_action:
          type: object
          description: Details on the action required to continue the run. Will be
            `null` if no action is required.
          nullable: true
          properties:
            type:
              description: For now, this is always `submit_tool_outputs`.
              type: string
              enum:
              - submit_tool_outputs
              x-stainless-const: true
            submit_tool_outputs:
              type: object
              description: Details on the tool outputs needed for this run to continue.
              properties:
                tool_calls:
                  type: array
                  description: A list of the relevant tool calls.
                  items:
                    $ref: '#/components/schemas/RunToolCallObject'
              required:
              - tool_calls
          required:
          - type
          - submit_tool_outputs
        last_error:
          type: object
          description: The last error associated with this run. Will be `null` if
            there are no errors.
          nullable: true
          properties:
            code:
              type: string
              description: One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`.
              enum:
              - server_error
              - rate_limit_exceeded
              - invalid_prompt
            message:
              type: string
              description: A human-readable description of the error.
          required:
          - code
          - message
        expires_at:
          description: The Unix timestamp (in seconds) for when the run will expire.
          type: integer
          nullable: true
        started_at:
          description: The Unix timestamp (in seconds) for when the run was started.
          type: integer
          nullable: true
        cancelled_at:
          description: The Unix timestamp (in seconds) for when the run was cancelled.
          type: integer
          nullable: true
        failed_at:
          description: The Unix timestamp (in seconds) for when the run failed.
          type: integer
          nullable: true
        completed_at:
          description: The Unix timestamp (in seconds) for when the run was completed.
          type: integer
          nullable: true
        incomplete_details:
          description: Details on why the run is incomplete. Will be `null` if the
            run is not incomplete.
          type: object
          nullable: true
          properties:
            reason:
              description: The reason why the run is incomplete. This will point to
                which specific token limit was reached over the course of the run.
              type: string
              enum:
              - max_completion_tokens
              - max_prompt_tokens
        model:
          description: The model that the [assistant](/docs/api-reference/assistants)
            used for this run.
          type: string
        instructions:
          description: The instructions that the [assistant](/docs/api-reference/assistants)
            used for this run.
          type: string
        tools:
          description: The list of tools that the [assistant](/docs/api-reference/assistants)
            used for this run.
          default: []
          type: array
          maxItems: 20
          items:
            oneOf:
            - $ref: '#/components/schemas/AssistantToolsCode'
            - $ref: '#/components/schemas/AssistantToolsFileSearch'
            - $ref: '#/components/schemas/AssistantToolsFunction'
            x-oaiExpandable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
        usage:
          $ref: '#/components/schemas/RunCompletionUsage'
        temperature:
          description: The sampling temperature used for this run. If not set, defaults
            to 1.
          type: number
          nullable: true
        top_p:
          description: The nucleus sampling value used for this run. If not set, defaults
            to 1.
          type: number
          nullable: true
        max_prompt_tokens:
          type: integer
          nullable: true
          description: 'The maximum number of prompt tokens specified to have been
            used over the course of the run.

            '
          minimum: 256
        max_completion_tokens:
          type: integer
          nullable: true
          description: 'The maximum number of completion tokens specified to have
            been used over the course of the run.

            '
          minimum: 256
        truncation_strategy:
          allOf:
          - $ref: '#/components/schemas/TruncationObject'
          - nullable: true
        tool_choice:
          allOf:
          - $ref: '#/components/schemas/AssistantsApiToolChoiceOption'
          - nullable: true
        parallel_tool_calls:
          $ref: '#/components/schemas/ParallelToolCalls'
        response_format:
          $ref: '#/components/schemas/AssistantsApiResponseFormatOption'
          nullable: true
      required:
      - id
      - object
      - created_at
      - thread_id
      - assistant_id
      - status
      - required_action
      - last_error
      - expires_at
      - started_at
      - cancelled_at
      - failed_at
      - completed_at
      - model
      - instructions
      - tools
      - metadata
      - usage
      - incomplete_details
      - max_prompt_tokens
      - max_completion_tokens
      - truncation_strategy
      - tool_choice
      - parallel_tool_calls
      - response_format
      x-oaiMeta:
        name: The run object
        beta: true
        example: "{\n  \"id\": \"run_abc123\",\n  \"object\": \"thread.run\",\n  \"\
          created_at\": 1698107661,\n  \"assistant_id\": \"asst_abc123\",\n  \"thread_id\"\
          : \"thread_abc123\",\n  \"status\": \"completed\",\n  \"started_at\": 1699073476,\n\
          \  \"expires_at\": null,\n  \"cancelled_at\": null,\n  \"failed_at\": null,\n\
          \  \"completed_at\": 1699073498,\n  \"last_error\": null,\n  \"model\":\
          \ \"gpt-4o\",\n  \"instructions\": null,\n  \"tools\": [{\"type\": \"file_search\"\
          }, {\"type\": \"code_interpreter\"}],\n  \"metadata\": {},\n  \"incomplete_details\"\
          : null,\n  \"usage\": {\n    \"prompt_tokens\": 123,\n    \"completion_tokens\"\
          : 456,\n    \"total_tokens\": 579\n  },\n  \"temperature\": 1.0,\n  \"top_p\"\
          : 1.0,\n  \"max_prompt_tokens\": 1000,\n  \"max_completion_tokens\": 1000,\n\
          \  \"truncation_strategy\": {\n    \"type\": \"auto\",\n    \"last_messages\"\
          : null\n  },\n  \"response_format\": \"auto\",\n  \"tool_choice\": \"auto\"\
          ,\n  \"parallel_tool_calls\": true\n}\n"
    RunToolCallObject:
      type: object
      description: Tool call objects
      properties:
        id:
          type: string
          description: The ID of the tool call. This ID must be referenced when you
            submit the tool outputs in using the [Submit tool outputs to run](/docs/api-reference/runs/submitToolOutputs)
            endpoint.
        type:
          type: string
          description: The type of tool call the output is required for. For now,
            this is always `function`.
          enum:
          - function
          x-stainless-const: true
        function:
          type: object
          description: The function definition.
          properties:
            name:
              type: string
              description: The name of the function.
            arguments:
              type: string
              description: The arguments that the model expects you to pass to the
                function.
          required:
          - name
          - arguments
      required:
      - id
      - type
      - function
    StaticChunkingStrategy:
      type: object
      additionalProperties: false
      properties:
        max_chunk_size_tokens:
          type: integer
          minimum: 100
          maximum: 4096
          description: The maximum number of tokens in each chunk. The default value
            is `800`. The minimum value is `100` and the maximum value is `4096`.
        chunk_overlap_tokens:
          type: integer
          description: 'The number of tokens that overlap between chunks. The default
            value is `400`.


            Note that the overlap must not exceed half of `max_chunk_size_tokens`.

            '
      required:
      - max_chunk_size_tokens
      - chunk_overlap_tokens
    StaticChunkingStrategyRequestParam:
      type: object
      title: Static Chunking Strategy
      description: Customize your own chunking strategy by setting chunk size and
        chunk overlap.
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `static`.
          enum:
          - static
          x-stainless-const: true
        static:
          $ref: '#/components/schemas/StaticChunkingStrategy'
      required:
      - type
      - static
    StaticChunkingStrategyResponseParam:
      type: object
      title: Static Chunking Strategy
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `static`.
          enum:
          - static
          x-stainless-const: true
        static:
          $ref: '#/components/schemas/StaticChunkingStrategy'
      required:
      - type
      - static
    StopConfiguration:
      description: 'Up to 4 sequences where the API will stop generating further tokens.
        The

        returned text will not contain the stop sequence.

        '
      default: null
      nullable: true
      oneOf:
      - type: string
        default: <|endoftext|>
        example: '

          '
        nullable: true
      - type: array
        minItems: 1
        maxItems: 4
        items:
          type: string
          example: '["\n"]'
    SubmitToolOutputsRunRequest:
      type: object
      additionalProperties: false
      properties:
        tool_outputs:
          description: A list of tools for which the outputs are being submitted.
          type: array
          items:
            type: object
            properties:
              tool_call_id:
                type: string
                description: The ID of the tool call in the `required_action` object
                  within the run object the output is being submitted for.
              output:
                type: string
                description: The output of the tool call to be submitted to continue
                  the run.
        stream:
          type: boolean
          nullable: true
          description: 'If `true`, returns a stream of events that happen during the
            Run as server-sent events, terminating when the Run enters a terminal
            state with a `data: [DONE]` message.

            '
      required:
      - tool_outputs
    ThreadObject:
      type: object
      title: Thread
      description: Represents a thread that contains [messages](/docs/api-reference/messages).
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `thread`.
          type: string
          enum:
          - thread
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the thread was created.
          type: integer
        tool_resources:
          type: object
          description: 'A set of resources that are made available to the assistant''s
            tools in this thread. The resources are specific to the type of tool.
            For example, the `code_interpreter` tool requires a list of file IDs,
            while the `file_search` tool requires a list of vector store IDs.

            '
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: 'A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a maximum
                    of 20 files associated with the tool.

                    '
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: 'The [vector store](/docs/api-reference/vector-stores/object)
                    attached to this thread. There can be a maximum of 1 vector store
                    attached to the thread.

                    '
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
      required:
      - id
      - object
      - created_at
      - tool_resources
      - metadata
      x-oaiMeta:
        name: The thread object
        beta: true
        example: "{\n  \"id\": \"thread_abc123\",\n  \"object\": \"thread\",\n  \"\
          created_at\": 1698107661,\n  \"metadata\": {}\n}\n"
    TruncationObject:
      type: object
      title: Thread Truncation Controls
      description: Controls for how a thread will be truncated prior to the run. Use
        this to control the intial context window of the run.
      properties:
        type:
          type: string
          description: The truncation strategy to use for the thread. The default
            is `auto`. If set to `last_messages`, the thread will be truncated to
            the n most recent messages in the thread. When set to `auto`, messages
            in the middle of the thread will be dropped to fit the context length
            of the model, `max_prompt_tokens`.
          enum:
          - auto
          - last_messages
        last_messages:
          type: integer
          description: The number of most recent messages from the thread when constructing
            the context for the run.
          minimum: 1
          nullable: true
      required:
      - type
    VectorStoreExpirationAfter:
      type: object
      title: Vector store expiration policy
      description: The expiration policy for a vector store.
      properties:
        anchor:
          description: 'Anchor timestamp after which the expiration policy applies.
            Supported anchors: `last_active_at`.'
          type: string
          enum:
          - last_active_at
          x-stainless-const: true
        days:
          description: The number of days after the anchor time that the vector store
            will expire.
          type: integer
          minimum: 1
          maximum: 365
      required:
      - anchor
      - days
    VectorStoreFileAttributes:
      type: object
      description: "Set of 16 key-value pairs that can be attached to an object. This\
        \ can be \nuseful for storing additional information about the object in a\
        \ structured \nformat, and querying for objects via API or the dashboard.\
        \ Keys are strings \nwith a maximum length of 64 characters. Values are strings\
        \ with a maximum \nlength of 512 characters, booleans, or numbers.\n"
      maxProperties: 16
      x-oaiTypeLabel: map
      nullable: true
    VectorStoreFileObject:
      type: object
      title: Vector store files
      description: A list of files attached to a vector store.
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `vector_store.file`.
          type: string
          enum:
          - vector_store.file
          x-stainless-const: true
        usage_bytes:
          description: The total vector store usage in bytes. Note that this may be
            different from the original file size.
          type: integer
        created_at:
          description: The Unix timestamp (in seconds) for when the vector store file
            was created.
          type: integer
        vector_store_id:
          description: The ID of the [vector store](/docs/api-reference/vector-stores/object)
            that the [File](/docs/api-reference/files) is attached to.
          type: string
        status:
          description: The status of the vector store file, which can be either `in_progress`,
            `completed`, `cancelled`, or `failed`. The status `completed` indicates
            that the vector store file is ready for use.
          type: string
          enum:
          - in_progress
          - completed
          - cancelled
          - failed
        last_error:
          type: object
          description: The last error associated with this vector store file. Will
            be `null` if there are no errors.
          nullable: true
          properties:
            code:
              type: string
              description: One of `server_error` or `rate_limit_exceeded`.
              enum:
              - server_error
              - unsupported_file
              - invalid_file
            message:
              type: string
              description: A human-readable description of the error.
          required:
          - code
          - message
        chunking_strategy:
          type: object
          description: The strategy used to chunk the file.
          oneOf:
          - $ref: '#/components/schemas/StaticChunkingStrategyResponseParam'
          - $ref: '#/components/schemas/OtherChunkingStrategyResponseParam'
          x-oaiExpandable: true
        attributes:
          $ref: '#/components/schemas/VectorStoreFileAttributes'
      required:
      - id
      - object
      - usage_bytes
      - created_at
      - vector_store_id
      - status
      - last_error
      x-oaiMeta:
        name: The vector store file object
        beta: true
        example: "{\n  \"id\": \"file-abc123\",\n  \"object\": \"vector_store.file\"\
          ,\n  \"usage_bytes\": 1234,\n  \"created_at\": 1698107661,\n  \"vector_store_id\"\
          : \"vs_abc123\",\n  \"status\": \"completed\",\n  \"last_error\": null,\n\
          \  \"chunking_strategy\": {\n    \"type\": \"static\",\n    \"static\":\
          \ {\n      \"max_chunk_size_tokens\": 800,\n      \"chunk_overlap_tokens\"\
          : 400\n    }\n  }\n}\n"
    VectorStoreObject:
      type: object
      title: Vector store
      description: A vector store is a collection of processed files can be used by
        the `file_search` tool.
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `vector_store`.
          type: string
          enum:
          - vector_store
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the vector store was
            created.
          type: integer
        name:
          description: The name of the vector store.
          type: string
        usage_bytes:
          description: The total number of bytes used by the files in the vector store.
          type: integer
        file_counts:
          type: object
          properties:
            in_progress:
              description: The number of files that are currently being processed.
              type: integer
            completed:
              description: The number of files that have been successfully processed.
              type: integer
            failed:
              description: The number of files that have failed to process.
              type: integer
            cancelled:
              description: The number of files that were cancelled.
              type: integer
            total:
              description: The total number of files.
              type: integer
          required:
          - in_progress
          - completed
          - failed
          - cancelled
          - total
        status:
          description: The status of the vector store, which can be either `expired`,
            `in_progress`, or `completed`. A status of `completed` indicates that
            the vector store is ready for use.
          type: string
          enum:
          - expired
          - in_progress
          - completed
        expires_after:
          $ref: '#/components/schemas/VectorStoreExpirationAfter'
        expires_at:
          description: The Unix timestamp (in seconds) for when the vector store will
            expire.
          type: integer
          nullable: true
        last_active_at:
          description: The Unix timestamp (in seconds) for when the vector store was
            last active.
          type: integer
          nullable: true
        metadata:
          $ref: '#/components/schemas/Metadata'
      required:
      - id
      - object
      - usage_bytes
      - created_at
      - status
      - last_active_at
      - name
      - file_counts
      - metadata
      x-oaiMeta:
        name: The vector store object
        example: "{\n  \"id\": \"vs_123\",\n  \"object\": \"vector_store\",\n  \"\
          created_at\": 1698107661,\n  \"usage_bytes\": 123456,\n  \"last_active_at\"\
          : 1698107661,\n  \"name\": \"my_vector_store\",\n  \"status\": \"completed\"\
          ,\n  \"file_counts\": {\n    \"in_progress\": 0,\n    \"completed\": 100,\n\
          \    \"cancelled\": 0,\n    \"failed\": 0,\n    \"total\": 100\n  },\n \
          \ \"last_used_at\": 1698107661\n}\n"
    WebSearchContextSize:
      type: string
      description: "High level guidance for the amount of context window space to\
        \ use for the \nsearch. One of `low`, `medium`, or `high`. `medium` is the\
        \ default.\n"
      enum:
      - low
      - medium
      - high
      default: medium
    WebSearchLocation:
      type: object
      title: Web search location
      description: Approximate location parameters for the search.
      properties:
        country:
          type: string
          description: "The two-letter \n[ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1)\
            \ of the user,\ne.g. `US`.\n"
        region:
          type: string
          description: 'Free text input for the region of the user, e.g. `California`.

            '
        city:
          type: string
          description: 'Free text input for the city of the user, e.g. `San Francisco`.

            '
        timezone:
          type: string
          description: "The [IANA timezone](https://timeapi.io/documentation/iana-timezones)\
            \ \nof the user, e.g. `America/Los_Angeles`.\n"
